{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b7f35828",
      "metadata": {
        "id": "b7f35828"
      },
      "source": [
        "# LLM Model Zoo: How to Choose and Compare Language Models\n",
        "\n",
        "This notebook covers the landscape of LLMs, how to evaluate them, and selecting the right (open-source) model for your use case.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hyperskill-content/hyperskill-ml-notebooks/blob/main/Attention_developments/llm_model_zoo.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5a0bc7f",
      "metadata": {
        "id": "b5a0bc7f"
      },
      "source": [
        "## ðŸš€ Prerequisites\n",
        "\n",
        "Make sure you're comfortable with the topics below before starting this notebook:\n",
        "\n",
        "| # | Topic (clickable links)|\n",
        "|---|-------|\n",
        "| 1 | **[Introduction to LLMs](https://hyperskill.org/learn/step/51833)**  |\n",
        "| 2 | **[Attention Mechanisms in Transformers](https://hyperskill.org/learn/step/52060)** |\n",
        "\n",
        "\n",
        "*If you're new to any item above, review it quickly, then dive back in here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1429e99",
      "metadata": {
        "id": "f1429e99"
      },
      "source": [
        "## 1. What's the Idea Behind Model Zoos? <a id=\"sec-idea\"></a>\n",
        "\n",
        "An LLM model zoo is a collection or repository of available large language models that users can browse, compare, and select from based on their specific needs\n",
        "\n",
        "### Why Do We Need So Many Models?\n",
        "\n",
        "| **The Problem** | **The Solution** |\n",
        "|-----------------|------------------|\n",
        "| One model can't be the best on all tasks | Different models for different tasks |\n",
        "| Some tasks need speed, others need quality | Fast models vs. accurate models |\n",
        "| Not everyone has the required hardware | Small models for laptops, big models for servers |\n",
        "| Different languages and cultures | Specialized models for specific regions |\n",
        "\n",
        "**Model Zoo** = A collection of pre-trained models you can choose from."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9894e5e",
      "metadata": {
        "id": "b9894e5e"
      },
      "source": [
        "## 2. Understanding the Open LLM Leaderboard <a id=\"sec-leaderboard\"></a>\n",
        "\n",
        "The [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/) evaluates models on multiple tasks and gives them an overall score. It only includes open-source models (no GPT-4, Claude, etc.), models available on Hugging Face, and reproducible results with standardized benchmarks.\n",
        "\n",
        "### **Key Evaluation Tasks**\n",
        "\n",
        "The leaderboard uses the following benchmarks:\n",
        "\n",
        "| Benchmark | Description |\n",
        "|-----------|-------------|\n",
        "| **IFEval** | Measures how well models follow specific instructions and formatting requirements. Tests adherence to constraints like word limits, specific formats, or structural requirements rather than just providing good content. |\n",
        "| **BBH** | Evaluates complex reasoning across challenging tasks from the BIG-Bench collection. Focuses on problems requiring multi-step reasoning, planning, and sophisticated cognitive abilities. |\n",
        "| **MATH** | Assesses mathematical problem-solving capabilities through competition-level mathematics problems. Requires step-by-step reasoning across algebra, geometry, and calculus. |\n",
        "| **GPQA** | Tests expert-level knowledge in science fields including biology, chemistry, and physics. Questions are designed to be difficult enough that even experts with Google access would find them challenging. |\n",
        "| **MUSR** | Evaluates multi-step soft reasoning that requires multiple inference steps and handling of ambiguous or incomplete information. Tests nuanced thinking used in everyday problem-solving. |\n",
        "| **MMLU-PRO** | Enhanced version of MMLU with more challenging graduate-level questions across 14 subject areas. Features harder questions and better answer choices to avoid saturation issues. |\n",
        "\n",
        "### **How to Read the Leaderboard**\n",
        "\n",
        "**Step-by-step guide:**\n",
        "\n",
        "1. **Overall Score**: Higher = better (usually 0-100 scale)\n",
        "2. **Model Size**: Look at parameters (7B, 13B, 70B, etc.)\n",
        "3. **License**: Check if you can use it commercially\n",
        "4. **Date**: Newer models often perform better\n",
        "\n",
        "**Pro tip**: Don't just look at the top score! Consider:\n",
        "- **Size vs Performance**: A 7B model scoring 75 might be better than a 70B model scoring 80 for your use case\n",
        "- **Task-specific scores**: If you need math, look at GSM8k specifically\n",
        "- **Your hardware**: Can you actually run the top model?\n",
        "\n",
        "### **Using the Leaderboard for Model Selection**\n",
        "\n",
        "**Example**: You need a model for customer support.\n",
        "\n",
        "1. **Go to**: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard/\n",
        "2. **Filter by size**: If you have 16GB RAM, look at 7B-8B models\n",
        "3. **Pick top 3 candidates**: Maybe Qwen2.5-7B, Llama-3.1-8B, Mistral-Nemo-12B\n",
        "4. **Test them**: Download and compare on your specific task\n",
        "\n",
        "### **Leaderboard Limitations**\n",
        "\n",
        "**Remember**: The leaderboard is helpful but not perfect!\n",
        "\n",
        "- **Doesn't test everything**: No coding, multilingual, or creative writing benchmarks\n",
        "- **Standardized tests**: Real-world performance might differ\n",
        "- **Gaming possible**: Some models might be specifically trained for these tests\n",
        "- **No commercial models**: Can't compare with GPT, Claude directly\n",
        "\n",
        "**Best practice**: Use the leaderboard as a starting point, then test models on your specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "146ff361",
      "metadata": {
        "id": "146ff361"
      },
      "source": [
        "## 3. Model Selection: Size, Hardware & Performance <a id=\"sec-selection\"></a>\n",
        "\n",
        "Choosing the right model isn't just about performance - you need to consider **hardware requirements**, **inference speed**, and **practical constraints**. Let's break this down systematically!\n",
        "\n",
        "### **Hardware Requirements by Model Size**\n",
        "\n",
        "**Memory requirements** (approximate values for inference):\n",
        "\n",
        "| Model Size | RAM Needed | GPU Memory | Examples | Best For |\n",
        "|------------|------------|------------|----------|----------|\n",
        "| **1-3B** | 4-8GB | 6-8GB | Phi-3-mini, Llama-3.2-1B | Laptops, mobile, edge |\n",
        "| **7-8B** | 16-32GB | 12-16GB | Llama-3.1-8B, Qwen2.5-7B | Workstations, small servers |\n",
        "| **13-14B** | 32-64GB | 24-32GB | Qwen2.5-14B, Phi-3-medium | Professional workstations |\n",
        "| **30-40B** | 64-128GB | 48-80GB | Mixtral-8x7B (MoE efficiency) | Server clusters |\n",
        "| **70B+** | 128GB+ | 80GB+ | Llama-3.1-70B, Qwen2.5-72B | Data center, cloud |\n",
        "\n",
        "### **Performance Considerations**\n",
        "\n",
        "**Inference Speed Factors:**\n",
        "1. **Model size**: Bigger = slower (generally)\n",
        "2. **Architecture**: MoE models (Mixtral) can be faster than expected\n",
        "3. **Hardware**: GPU > CPU, newer GPUs > older\n",
        "4. **Optimization**: Quantization, optimized libraries (vLLM, TGI)\n",
        "5. **Batch size**: Multiple requests together = more efficient\n",
        "\n",
        "### **The Hardware-Performance Matrix**\n",
        "\n",
        "| Your Hardware | Recommended Models | Expected Performance |\n",
        "|---------------|-------------------|---------------------|\n",
        "| **Laptop (8-16GB RAM)** | Phi-3-mini, Qwen2.5-3B | Good for simple tasks, 2-5 tokens/sec |\n",
        "| **Workstation (32GB RAM, RTX 4090)** | Llama-3.1-8B, Qwen2.5-7B | Excellent for most tasks, 20-50 tokens/sec |\n",
        "| **Server (64GB RAM, A100)** | Qwen2.5-14B, Mixtral-8x7B | Professional quality, 50-100 tokens/sec |\n",
        "| **Cloud/Cluster (Multi-GPU)** | Llama-3.1-70B, Qwen2.5-72B | Top-tier quality, 100+ tokens/sec |\n",
        "\n",
        "### **Optimization Techniques**\n",
        "\n",
        "**Make models run faster and use less memory:**\n",
        "\n",
        "1. **Quantization**: Reduce precision (FP16, INT8, INT4)\n",
        "2. **Model Optimization Libraries**:\n",
        "   - **vLLM**: Fast inference server\n",
        "   - **TensorRT-LLM**: NVIDIA GPU acceleration\n",
        "   - **Ollama**: Easy local deployment\n",
        "   - **llama.cpp**: CPU-optimized inference\n",
        "\n",
        "3. **Hardware-Specific Optimizations**:\n",
        "   - **Apple Silicon**: Use MLX for M1/M2/M3 Macs\n",
        "   - **NVIDIA GPUs**: Use CUDA, TensorRT\n",
        "   - **AMD GPUs**: Use ROCm\n",
        "   - **Intel CPUs**: Use optimized BLAS libraries\n",
        "\n",
        "### **Model Selection Decision Tree**\n",
        "\n",
        "```\n",
        "What hardware do you have?\n",
        "â”œâ”€â”€ Laptop (8-16GB) â†’ Phi-3-mini, Qwen2.5-3B, or use APIs\n",
        "â”œâ”€â”€ Workstation (RTX 3090/4090) â†’ Llama-3.1-8B, Qwen2.5-7B\n",
        "â”œâ”€â”€ Server (A100/H100) â†’ Qwen2.5-14B, Mixtral-8x7B\n",
        "â””â”€â”€ Multi-GPU cluster â†’ Llama-3.1-70B, Qwen2.5-72B\n",
        "\n",
        "What's your primary task?\n",
        "â”œâ”€â”€ General chat â†’ Llama-3.1 series\n",
        "â”œâ”€â”€ Multilingual â†’ Qwen2.5 series  \n",
        "â”œâ”€â”€ Coding â†’ Qwen2.5-Coder or Code Llama\n",
        "â”œâ”€â”€ Math/reasoning â†’ Qwen2.5-Math or high-scoring MMLU models\n",
        "â””â”€â”€ Efficiency focus â†’ Phi-3 or Mixtral series\n",
        "\n",
        "How much can you spend on hardware?\n",
        "â”œâ”€â”€ $0-500 â†’ Use smaller models or cloud APIs\n",
        "â”œâ”€â”€ $500-3000 â†’ RTX 4090 + 7B-8B models\n",
        "â”œâ”€â”€ $3000-10000 â†’ Workstation + 14B models\n",
        "â””â”€â”€ $10000+ â†’ Server-grade + 70B+ models\n",
        "```\n",
        "\n",
        "### **Practical Tips for Model Selection**\n",
        "\n",
        "1. **Start small**: Begin with 7B-8B models, upgrade only if needed\n",
        "2. **Check the leaderboard**: Compare models of similar size\n",
        "3. **Test on your data**: Leaderboard scores don't guarantee good performance on your task\n",
        "4. **Consider fine-tuning**: A smaller fine-tuned model often beats a larger general model\n",
        "5. **Factor in deployment**: Easier deployment might be worth slight performance loss\n",
        "\n",
        "### **Real-World Example**\n",
        "\n",
        "**Scenario**: Building a customer support chatbot\n",
        "\n",
        "1. **Check leaderboard**: Qwen2.5-7B scores 76.9 (good), Llama-3.1-8B scores 77.4 (slightly better)\n",
        "2. **Check hardware**: You have RTX 4090 (24GB) - both models will fit\n",
        "3. **Consider specialization**: Customer support needs good instruction following and multilingual support\n",
        "4. **Decision**: Try both models, but Qwen2.5-7B might be better for multilingual customers\n",
        "5. **Optimization**: Use GGUF quantization to speed up inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff0e010",
      "metadata": {
        "id": "aff0e010"
      },
      "source": [
        "## 4. Deep Dive: Major Open-Source Families <a id=\"sec-deep-dive\"></a>\n",
        "\n",
        "Analysis of major open-source model families, including specifications and optimal use cases.\n",
        "\n",
        "### Llama (Meta)\n",
        "\n",
        "The Llama model family ranges from 1B to 405B parameters across versions 3.1 and 3.2. It offers strong instruction following and general knowledge capabilities, with Llama 3.2 adding vision support and mobile-optimized variants. The family has the largest open-source community and uses the Llama Community License for commercial applications. Llama models are widely used for both research and commercial deployments due to their reliable performance and extensive ecosystem support.\n",
        "\n",
        "**Limitations:**\n",
        "- Primarily English-focused\n",
        "- Large models require significant computational resources\n",
        "- License restrictions for very large deployments\n",
        "\n",
        "**Best Use Cases:**\n",
        "- General-purpose applications\n",
        "- Domain-specific fine-tuning\n",
        "- Commercial products requiring reliable performance\n",
        "- Applications requiring strong instruction adherence\n",
        "\n",
        "**Hugging Face Models:** https://huggingface.co/meta-llama/collections\n",
        "\n",
        "---\n",
        "\n",
        "### Qwen (Alibaba)\n",
        "\n",
        "The Qwen model family is a series of LLMs ranging from 0.5B to 235B parameters across multiple generations (1.0, 1.5, 2.5, and 3.0). The family excels in multilingual capabilities, particularly Chinese and English, and offers specialized variants for coding (Qwen-Coder), mathematics (Qwen-Math), reasoning (QwQ), and multimodal tasks (Qwen-VL). The latest Qwen 3.0 series introduces efficient MoE architecture and advanced reasoning capabilities, making it competitive with leading commercial models.\n",
        "\n",
        "**Limitations:**\n",
        "- Smaller community compared to Llama\n",
        "- Documentation often primarily in Chinese\n",
        "- Newer family with less established ecosystem\n",
        "\n",
        "**Best Use Cases:**\n",
        "- Multilingual applications\n",
        "- Software development and coding assistance\n",
        "- Mathematical and scientific reasoning tasks\n",
        "- Research projects requiring permissive licensing\n",
        "\n",
        "**Hugging Face Models:** https://huggingface.co/Qwen/collections\n",
        "\n",
        "---\n",
        "\n",
        "### Mixtral (Mistral AI)\n",
        "The Mixtral model family uses Mixture-of-Experts (MoE) architecture with models like Mixtral 8x7B (47B total, 13B active) and 8x22B (141B total, 39B active), plus the dense Mistral NeMo 12B model. The family provides efficient inference by activating only a subset of parameters per token while maintaining large model performance, with context lengths up to 128K tokens. Mixtral models are designed for European regulatory compliance and offer strong reasoning capabilities. They are suitable for high-throughput deployments where computational efficiency and regulatory compliance are priorities.\n",
        "\n",
        "**Limitations:**\n",
        "- MoE deployment complexity requiring specialized infrastructure\n",
        "- High memory requirements despite efficiency gains\n",
        "- Smaller ecosystem than Llama\n",
        "\n",
        "**Best Use Cases:**\n",
        "- High-throughput serving requiring efficiency\n",
        "- European deployments needing regulatory compliance\n",
        "- Long document processing applications\n",
        "- Reasoning-intensive tasks\n",
        "\n",
        "**Hugging Face Models:** https://huggingface.co/mistralai/models\n",
        "\n",
        "---\n",
        "\n",
        "### Phi (Microsoft)\n",
        "\n",
        "The Phi model family, developed by Microsoft, includes Phi-3 (3.8B-14B parameters) and the newer Phi-4 (14B parameters specializing in complex reasoning and mathematics). Additional variants include Phi-4-mini with enhanced multilingual support and Phi-4-multimodal for vision and audio processing. The family achieves high performance per parameter through training on curated synthetic data and is optimized for mobile and edge deployment with 128K context length. Licensed under MIT, Phi models offer the most permissive commercial licensing and are designed for resource-constrained environments where efficiency is prioritized over absolute performance.\n",
        "\n",
        "**Limitations:**\n",
        "- Notable weakness in following complex or specific instructions\n",
        "- Limited multilingual support\n",
        "- Smaller community and ecosystem\n",
        "\n",
        "**Best Use Cases:**\n",
        "- Mobile and edge applications\n",
        "- Resource-constrained environments\n",
        "- Applications prioritizing model size over absolute performance\n",
        "- Prototyping and proof-of-concept development\n",
        "\n",
        "**Hugging Face Models:** https://huggingface.co/microsoft/collections\n",
        "\n",
        "---\n",
        "\n",
        "### Family Comparison\n",
        "\n",
        "| Feature | Llama | Qwen | Mixtral | Phi |\n",
        "|---------|-------|------|---------|-------|\n",
        "| **Primary Use** | General purpose | Multilingual | Efficiency | Edge/Mobile |\n",
        "| **License** | Custom (commercial OK) | Apache 2.0 | Apache 2.0 | MIT |\n",
        "| **Multilingual** | Good | Excellent | Good | Limited |\n",
        "| **Community** | Largest | Growing | Medium | Small |\n",
        "| **Hardware Requirements** | High (large models) | Medium-High | Medium | Low |\n",
        "| **Deployment Complexity** | Low | Low | High (MoE) | Very Low |\n",
        "\n",
        "### Selection Guidelines\n",
        "\n",
        "**Choose Llama for**: Established ecosystem, general-purpose applications, large community support    \n",
        "**Choose Qwen for**: Multilingual requirements, coding/mathematics tasks, permissive licensing      \n",
        "**Choose Mixtral for**: Efficiency-focused deployments, EU compliance, reasoning tasks    \n",
        "**Choose Phi for**: Resource constraints, edge deployment, mobile applications"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c7170ae",
      "metadata": {
        "id": "3c7170ae"
      },
      "source": [
        "### Evaluation Resources\n",
        "\n",
        "**Performance Benchmarks**:\n",
        "- LMSYS Chatbot Arena: https://chat.lmsys.org/?leaderboard\n",
        "- Hugging Face Open LLM Leaderboard: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard/\n",
        "- Artificial Analysis: https://artificialanalysis.ai/\n",
        "\n",
        "**Model Access**:\n",
        "- Hugging Face Hub: https://huggingface.co/models\n",
        "- Ollama: https://ollama.ai/library\n",
        "- Together AI: https://www.together.ai/\n",
        "\n",
        "Note: Model rankings change frequently. Check current benchmarks before making decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "912b1e08",
      "metadata": {
        "id": "912b1e08"
      },
      "source": [
        "## 5. Hands-on: Working with Hugging Face Models <a id=\"sec-hands-on\"></a>\n",
        "\n",
        "Time to get practical! We'll explore how to discover, load, and compare different Hugging Face models. This section focuses on **open-source models only**.\n",
        "\n",
        "### **Discovering Models on Hugging Face**\n",
        "\n",
        "**Step 1: Using the Hugging Face Hub**\n",
        "- Go to https://huggingface.co/models\n",
        "- Filter by task (text-generation, text-classification, etc.)\n",
        "- Sort by trending, most downloads, or recently updated\n",
        "- Check model cards for details, examples, and benchmarks\n",
        "\n",
        "**Step 2: Reading Model Cards**        \n",
        "Every model has a **model card** with **performance metrics** (scores on benchmarks), **hardware requirements** (memory, GPU needs), **usage examples** (code to get started), **license information** (commercial use allowed?), **intended use cases** (what it's good/bad at)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0768254f",
      "metadata": {
        "id": "0768254f"
      },
      "source": [
        "#### **Example 1: Llama Family - General Purpose Chat**\n",
        "\n",
        "**Model**: `meta-llama/Llama-3.2-1B-Instruct` (1B parameters)\n",
        "- **Memory needed**: ~2-4GB\n",
        "- **Best for**: General conversation, instruction following\n",
        "- **Hardware**: Runs on laptops, CPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ae9388",
      "metadata": {
        "id": "96ae9388"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Example 1: TinyLlama\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"cpu\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Explain quantum computing in simple terms.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5393dba8",
      "metadata": {
        "id": "5393dba8"
      },
      "source": [
        "#### **Example 2: Qwen Family - Multilingual & Math**\n",
        "\n",
        "**Model**: `Qwen/Qwen2.5-0.5B-Instruct` (0.5B parameters)\n",
        "- **Memory needed**: ~1GB\n",
        "- **Best for**: Multilingual tasks, basic math, coding\n",
        "- **Special**: Excellent for non-English languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a734e45",
      "metadata": {
        "id": "5a734e45"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Example 2: Qwen2.5\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"cpu\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Give me a short introduction to large language models.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c0da5c4",
      "metadata": {
        "id": "3c0da5c4"
      },
      "source": [
        "#### **Example 3: Phi-3 Family - Efficient & Mobile-Ready**\n",
        "\n",
        "**Model**: `microsoft/Phi-3-mini-4k-instruct` (3.8B parameters)\n",
        "- **Memory needed**: ~4-8GB\n",
        "- **Best for**: Efficient inference, mobile deployment, reasoning\n",
        "- **Special**: Great performance per parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f822fd98",
      "metadata": {
        "id": "f822fd98"
      },
      "source": [
        "#### **Model Comparison Function**\n",
        "\n",
        "Let's create a function to compare multiple models side-by-side on the same task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00b7ebd6",
      "metadata": {
        "id": "00b7ebd6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Model Comparison\n",
        "models_to_compare = [\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "]\n",
        "\n",
        "prompt = \"What are the benefits of renewable energy?\"\n",
        "results = {}\n",
        "\n",
        "for model_name in models_to_compare:\n",
        "    try:\n",
        "        print(f\"\\nTesting {model_name}...\")\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=\"auto\",\n",
        "            device_map=\"cpu\"\n",
        "        )\n",
        "\n",
        "        # Simple generation for DialoGPT, chat template for instruct models\n",
        "        if \"DialoGPT\" in model_name:\n",
        "            inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "            outputs = model.generate(inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n",
        "        else:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
        "\n",
        "            generated_ids = model.generate(**model_inputs, max_new_tokens=200)\n",
        "            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
        "            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        results[model_name] = response.strip()\n",
        "        print(f\"Response: {response.strip()}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {model_name}: {str(e)}\")\n",
        "        results[model_name] = f\"Error: {str(e)}\"\n",
        "\n",
        "print(f\"\\nComparison completed for {len(results)} models.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02cf266f",
      "metadata": {
        "id": "02cf266f"
      },
      "source": [
        "### **Performance Optimization Tips**\n",
        "\n",
        "If you're running into memory issues, here are some optimization techniques:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb9a80a",
      "metadata": {
        "id": "3bb9a80a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"\\nChecking available optimizations...\")\n",
        "\n",
        "\n",
        "try:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    print(\"BitsAndBytesConfig available for quantization\")\n",
        "    quantization_available = True\n",
        "except ImportError:\n",
        "    print(\"BitsAndBytesConfig not available\")\n",
        "    print(\"Install with: pip install bitsandbytes\")\n",
        "    quantization_available = False\n",
        "\n",
        "print(\"\\nDemonstrating memory-efficient model loading:\")\n",
        "\n",
        "try:\n",
        "    # Use a very small model for demonstration\n",
        "    model_name = \"distilgpt2\"  # Small, widely compatible model\n",
        "\n",
        "    print(f\"Loading {model_name} with memory optimizations...\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Method 1: Low CPU memory usage\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float32,  # Compatible dtype\n",
        "        device_map=\"cpu\",  # CPU only for compatibility\n",
        "        low_cpu_mem_usage=True,  # Memory optimization\n",
        "        # use_cache=False,  # Reduce memory during generation\n",
        "    )\n",
        "\n",
        "    print(\"Model loaded with memory optimizations!\")\n",
        "\n",
        "    # Quick test\n",
        "    test_text = \"Artificial intelligence is\"\n",
        "    inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=15,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Test output: {result}\")\n",
        "\n",
        "    # Clean up\n",
        "    del model, tokenizer\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Memory optimization demo failed: {e}\")\n",
        "\n",
        "# Example 3: Using Pipeline for efficiency\n",
        "print(\"\\nUsing Pipeline for efficient inference:\")\n",
        "\n",
        "try:\n",
        "    # Pipeline is often more memory efficient for simple tasks\n",
        "    from transformers import pipeline\n",
        "\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=\"distilgpt2\",\n",
        "        torch_dtype=torch.float32,\n",
        "        device=\"cpu\",  # CPU for compatibility\n",
        "        model_kwargs={\"low_cpu_mem_usage\": True}\n",
        "    )\n",
        "\n",
        "    result = generator(\n",
        "        \"The future of AI is\",\n",
        "        max_new_tokens=20,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    print(\"Pipeline created successfully!\")\n",
        "    print(f\"Pipeline output: {result[0]['generated_text']}\")\n",
        "\n",
        "    del generator\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Pipeline demo failed: {e}\")\n",
        "\n",
        "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "print(\"\\nMemory cleaned up.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f5bcf4e",
      "metadata": {
        "id": "0f5bcf4e"
      },
      "source": [
        "### **Model Comparison Summary**\n",
        "\n",
        "| Model | Size | Memory | Strengths | Best Use Cases |\n",
        "|-------|------|---------|-----------|---------------|\n",
        "| **Llama-3.2-1B** | 1B | 2-4GB | General purpose, instruction following | Chatbots, general tasks |\n",
        "| **Qwen2.5-1.5B** | 1.5B | 3-6GB | Multilingual, math, coding | Global apps, education |\n",
        "\n",
        "**Remember**: These lightweight models are perfect for learning, prototyping, and many production use cases. Start here before moving to larger models!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
