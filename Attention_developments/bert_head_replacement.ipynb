{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b7fcb86e",
      "metadata": {
        "id": "b7fcb86e"
      },
      "source": [
        "# BERT Head Replacement: Adapting Pre-trained Models for Custom Tasks\n",
        "\n",
        "In this notebook you will learn about head replacement - a technique to adapt pre-trained BERT models for specific tasks by replacing the final classification layer while preserving the pre-trained representations.\n",
        "You'll learn how BERT works, why head replacement is effective, and how to implement it with practical examples.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hyperskill-content/hyperskill-ml-notebooks/blob/main/Attention_developments/bert_head_replacement.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f490eef5",
      "metadata": {
        "id": "f490eef5"
      },
      "source": [
        "## üöÄ Prerequisites\n",
        "\n",
        "Make sure you're comfortable with the topics below before starting this notebook:\n",
        "\n",
        "| # | Topic (clickable links)|\n",
        "|---|-------|\n",
        "| 1 | **[Transformers in NLP](https://hyperskill.org/learn/step/30103)**  |\n",
        "| 2 | **[Tokenization](https://hyperskill.org/learn/step/51949)** |\n",
        "| 3 | **[BERT architecture](https://lena-voita.github.io/nlp_course/transfer_learning.html#bert)** |\n",
        "\n",
        "\n",
        "*If you're new to any item above, review it quickly, then dive back in here.*\n",
        "\n",
        "For this notebook, enable the hosted GPU runtime in Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01307b74",
      "metadata": {
        "id": "01307b74"
      },
      "source": [
        "# üìë Table of Contents  \n",
        "  \n",
        "1. [What's the Big Idea Behind Head Replacement?](#sec-idea)  \n",
        "2. [BERT Architecture Primer](#sec-bert-primer)  \n",
        "3. [Base Model vs Head: The Conceptual Split](#sec-base-vs-head)\n",
        "4. [Head Replacement vs Fine-tuning: When to Use What?](#sec-comparison)  \n",
        "5. [What You Need for Head Replacement](#sec-requirements)\n",
        "6. [Hands-on: Classification Head Replacement](#sec-classification)  \n",
        "7. [Hands-on: Question Answering Head](#sec-qa)\n",
        "8. [Hands-on: Token Classification Head](#sec-token-class)  \n",
        "9. [Performance Comparison & Best Practices](#sec-best-practices)\n",
        "10. [Practice Exercises](#sec-exercises)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "981d4a7c",
      "metadata": {
        "id": "981d4a7c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Check if we have GPU available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Current date: {datetime.now().strftime('%Y-%m-%d')}\")\n",
        "print(\"Transformers library loaded!\")\n",
        "print(\"Ready to explore BERT head replacement!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3629a7f",
      "metadata": {
        "id": "e3629a7f"
      },
      "source": [
        "## 1. What's the Big Idea Behind Head Replacement? <a id=\"sec-idea\"></a>\n",
        "\n",
        "**Head replacement** is a technique where you keep BERT's pre-trained layers frozen and only train a new head for your specific task.    \n",
        "**Traditional fine-tuning**: Updates all model parameters using task-specific data, which can alter the pre-trained representations.    \n",
        "**Head replacement**: Keeps the pre-trained model layers unchanged and only trains the final classification layer.\n",
        "\n",
        "### Advantages of Head Replacement\n",
        "\n",
        "| **Advantage** | **Explanation** |\n",
        "|---------------|-----------------|\n",
        "| **Speed** | Only training the head reduces computation time |\n",
        "| **Data Efficiency** | Requires fewer task-specific examples |\n",
        "| **Stability** |  Preserves pre-trained language representations |\n",
        "| **Multiple Tasks** | Same base model can support multiple task-specific heads |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35545e68",
      "metadata": {
        "id": "35545e68"
      },
      "source": [
        "## 2. BERT Architecture Primer <a id=\"sec-bert-primer\"></a>\n",
        "**BERT = Bidirectional Encoder Representations from Transformers**\n",
        "\n",
        "**Bidirectional**: Processes text in both directions simultaneously, considering context from both left and right         \n",
        "**Encoder**: Uses the encoder component of the Transformer architecture to create text representations    \n",
        "**Representations**: Generates numerical embeddings that capture semantic meaning          \n",
        "**Transformers**: Built on the attention-based Transformer architecture   \n",
        "### BERT's Architecture in Simple Terms\n",
        "\n",
        "```\n",
        "Input Text: \"The cat sat on the mat\"\n",
        "     ‚Üì\n",
        "[Tokenization] ‚Üí [\"[CLS]\", \"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"[SEP]\"]\n",
        "     ‚Üì\n",
        "[Embedding Layer] ‚Üí Convert tokens to vectors\n",
        "     ‚Üì\n",
        "[12 Transformer Layers] ‚Üí Deep understanding through attention\n",
        "     ‚Üì\n",
        "[Base Model Output] ‚Üí Rich representations for each token\n",
        "     ‚Üì\n",
        "[TASK HEAD] ‚Üê This is what we replace!\n",
        "     ‚Üì\n",
        "[Final Prediction]\n",
        "```\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **[CLS] Token**: Special token at the beginning - represents the entire sequence\n",
        "2. **[SEP] Token**: Separates different sentences or marks the end\n",
        "3. **Transformer Layers**: 12 layers (in BERT-base) that build understanding\n",
        "4. **Hidden Size**: 768 dimensions per token representation\n",
        "5. **Attention Heads**: 12 heads per layer that focus on different aspects\n",
        "\n",
        "![BERT Architecture](https://towardsdatascience.com/wp-content/uploads/2024/05/1Qww2aaIdqrWVeNmo3AS0ZQ.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9604fc",
      "metadata": {
        "id": "ff9604fc"
      },
      "outputs": [],
      "source": [
        "# Let's load a BERT model and explore its structure\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "print(\"BERT Model Structure:\")\n",
        "print(f\"Model name: {model_name}\")\n",
        "print(f\"Hidden size: {bert_model.config.hidden_size}\")\n",
        "print(f\"Number of layers: {bert_model.config.num_hidden_layers}\")\n",
        "print(f\"Number of attention heads: {bert_model.config.num_attention_heads}\")\n",
        "print(f\"Vocabulary size: {bert_model.config.vocab_size}\")\n",
        "\n",
        "# Let's see the actual model architecture\n",
        "print(\"\\nBERT Architecture Overview:\")\n",
        "print(bert_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32d62009",
      "metadata": {
        "id": "32d62009"
      },
      "outputs": [],
      "source": [
        "# Let's see how BERT processes text\n",
        "sample_text = \"The cat sat on the mat\"\n",
        "print(f\"Original text: '{sample_text}'\")\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(sample_text)\n",
        "print(f\"Tokens: {tokens}\")\n",
        "\n",
        "# Add special tokens and convert to IDs\n",
        "encoded = tokenizer.encode(sample_text, add_special_tokens=True)\n",
        "print(f\"Token IDs: {encoded}\")\n",
        "\n",
        "# Convert back to tokens to see special tokens\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded)\n",
        "print(f\"Tokens with special tokens: {decoded_tokens}\")\n",
        "\n",
        "# Get the actual text representations\n",
        "input_ids = torch.tensor([encoded])\n",
        "with torch.no_grad():\n",
        "    outputs = bert_model(input_ids)\n",
        "\n",
        "# The output contains representations for each token\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "print(f\"\\nOutput shape: {last_hidden_states.shape}\")\n",
        "print(f\"Shape explanation: [batch_size=1, sequence_length={len(encoded)}, hidden_size=768]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b1a2a26",
      "metadata": {
        "id": "3b1a2a26"
      },
      "source": [
        "## 3. Base Model vs Head: The Conceptual Split <a id=\"sec-base-vs-head\"></a>\n",
        "\n",
        "## 3. Base Model vs Head: The Conceptual Split <a id=\"sec-base-vs-head\"></a>\n",
        "\n",
        "BERT models can be conceptually divided into two components: the base model and the task head.\n",
        "\n",
        "### The Base Model\n",
        "\n",
        "The **base model** contains the core language understanding components:\n",
        "\n",
        "- All Transformer layers (12 in BERT-base)\n",
        "- Attention mechanisms that capture relationships between words\n",
        "- Pre-trained language representations learned from large text corpora\n",
        "\n",
        "During head replacement, the base model parameters are typically frozen to preserve the learned representations.\n",
        "\n",
        "### The Task Head\n",
        "\n",
        "The **task head** is a small neural network (usually 1-2 layers) that processes the base model's output for specific tasks:\n",
        "\n",
        "| **Task** | **Head Type** | **Function** |\n",
        "|----------|---------------|--------------|\n",
        "| **Text Classification** | Linear layer | Maps [CLS] token to class probabilities |\n",
        "| **Question Answering** | Two linear layers | Predicts start and end positions of answers |\n",
        "| **Token Classification** | Linear layer per token | Labels individual tokens (NER, POS tagging) |\n",
        "| **Similarity** | Cosine similarity | Compares sentence embeddings |\n",
        "\n",
        "### Why This Architecture Works\n",
        "\n",
        "The separation allows the base model to provide universal language understanding while the head handles task-specific predictions. This design enables efficient adaptation to new tasks without modifying the pre-trained representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9dd773",
      "metadata": {
        "id": "ff9dd773"
      },
      "outputs": [],
      "source": [
        "# Let's demonstrate the base model vs head concept\n",
        "class CustomClassificationHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple classification head that we can attach to BERT\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_classes, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, bert_output):\n",
        "        # Use the [CLS] token representation (first token)\n",
        "        cls_representation = bert_output.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
        "        cls_representation = self.dropout(cls_representation)\n",
        "        logits = self.classifier(cls_representation)\n",
        "        return logits\n",
        "\n",
        "class BERTWithCustomHead(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT base model + our custom head\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        super().__init__()\n",
        "        # Base model (we'll freeze this)\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "\n",
        "        # Custom head (we'll train this)\n",
        "        self.head = CustomClassificationHead(\n",
        "            hidden_size=self.bert.config.hidden_size,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "        # Freeze the base model\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        print(f\"üîí Frozen BERT parameters: {sum(p.numel() for p in self.bert.parameters())}\")\n",
        "        print(f\"üîì Trainable head parameters: {sum(p.numel() for p in self.head.parameters())}\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Get representations from base model\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Apply our custom head\n",
        "        logits = self.head(bert_output)\n",
        "        return logits\n",
        "\n",
        "# Let's create an example model for 3-class classification\n",
        "model_with_head = BERTWithCustomHead(model_name=\"bert-base-uncased\", num_classes=3)\n",
        "print(\"\\nCreated BERT with custom classification head!\")\n",
        "print(f\"Input size to head: {model_with_head.bert.config.hidden_size}\")\n",
        "print(f\"Output classes: 3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc7e0746",
      "metadata": {
        "id": "dc7e0746"
      },
      "source": [
        "## 4. Head Replacement vs Fine-tuning: When to Use What? <a id=\"sec-comparison\"></a>\n",
        "\n",
        "Head replacement and fine-tuning are two approaches for adapting BERT to specific tasks, each with distinct advantages.\n",
        "\n",
        "### Head Replacement\n",
        "\n",
        "**Process**: Train only the new task head while keeping the base model parameters frozen.\n",
        "\n",
        "**Best suited for**:\n",
        "- Small datasets (< 10k samples)\n",
        "- Rapid prototyping\n",
        "- Multiple tasks using the same base model\n",
        "- Preserving general language representations\n",
        "\n",
        "### Fine-tuning\n",
        "\n",
        "**Process**: Train the entire model including both base layers and task head.\n",
        "\n",
        "**Best suited for**:\n",
        "- Large datasets (> 50k samples)\n",
        "- Domain-specific tasks (medical, legal texts)\n",
        "- Maximum performance requirements\n",
        "- Tasks significantly different from BERT's pre-training\n",
        "\n",
        "### Comparison\n",
        "\n",
        "| **Aspect** | **Head Replacement** | **Full Fine-tuning** |\n",
        "|------------|---------------------|---------------------|\n",
        "| **Training Speed** | Fast (minutes) | Slower (hours) |\n",
        "| **Data Required** | Small (1k-10k) | Large (10k+) |\n",
        "| **Memory Usage** | Low | High |\n",
        "| **Risk of Overfitting** | Low | Higher |\n",
        "| **Performance** | Good | Best (with sufficient data) |\n",
        "| **Multiple Tasks** | Easy to manage | Requires separate models |\n",
        "\n",
        "### Selection Guidelines\n",
        "\n",
        "**Use Head Replacement when**:\n",
        "- Training data is limited\n",
        "- Quick experimentation is needed\n",
        "- Working with standard NLP tasks\n",
        "- Managing multiple task-specific models\n",
        "- Computational resources are constrained\n",
        "\n",
        "**Use Fine-tuning when**:\n",
        "- Abundant training data is available\n",
        "- Domain is highly specialized\n",
        "- Maximum performance is required\n",
        "- Sufficient computational resources are available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "833305b3",
      "metadata": {
        "id": "833305b3"
      },
      "outputs": [],
      "source": [
        "# Let's create a practical comparison showing the difference\n",
        "import torch.optim as optim\n",
        "\n",
        "def count_trainable_parameters(model):\n",
        "    \"\"\"Count how many parameters will be updated during training\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def create_head_replacement_model(num_classes):\n",
        "    \"\"\"Model with frozen BERT + trainable head\"\"\"\n",
        "    model = BERTWithCustomHead(\"bert-base-uncased\", num_classes)\n",
        "    return model\n",
        "\n",
        "def create_full_finetuning_model(num_classes):\n",
        "    \"\"\"Model where everything is trainable\"\"\"\n",
        "    model = BERTWithCustomHead(\"bert-base-uncased\", num_classes)\n",
        "\n",
        "    # Unfreeze all BERT parameters\n",
        "    for param in model.bert.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    return model\n",
        "\n",
        "# Compare the two approaches\n",
        "print(\"üîç Comparing Head Replacement vs Fine-tuning\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "head_model = create_head_replacement_model(num_classes=3)\n",
        "finetune_model = create_full_finetuning_model(num_classes=3)\n",
        "\n",
        "head_params = count_trainable_parameters(head_model)\n",
        "finetune_params = count_trainable_parameters(finetune_model)\n",
        "\n",
        "print(f\"üìä Head Replacement - Trainable parameters: {head_params:,}\")\n",
        "print(f\"üìä Full Fine-tuning - Trainable parameters: {finetune_params:,}\")\n",
        "print(f\"‚ö° Speed difference: {finetune_params / head_params:.1f}x more parameters to train\")\n",
        "\n",
        "# Memory usage estimation\n",
        "print(f\"\\nüíæ Approximate memory comparison:\")\n",
        "print(f\"Head Replacement: ~{head_params * 4 / 1e6:.1f} MB\")\n",
        "print(f\"Full Fine-tuning: ~{finetune_params * 4 / 1e6:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8c68801",
      "metadata": {
        "id": "a8c68801"
      },
      "source": [
        "## 5. Requirements for Head Replacement <a id=\"sec-requirements\"></a>\n",
        "\n",
        "### Data Requirements\n",
        "\n",
        "Data should be structured according to the target task:\n",
        "\n",
        "| **Task Type** | **Input Format** | **Output Format** | **Example** |\n",
        "|---------------|------------------|-------------------|-------------|\n",
        "| **Text Classification** | Text | Class label | \"I love this movie\" ‚Üí \"positive\" |\n",
        "| **Question Answering** | Context + Question | Start & End positions | \"Where is Paris?\" ‚Üí position 15-20 |\n",
        "| **Token Classification** | Text | Label per token | \"John lives in NYC\" ‚Üí [\"PERSON\", \"O\", \"O\", \"CITY\"] |\n",
        "\n",
        "### Technical Requirements\n",
        "\n",
        "1. **Base Model**: Select appropriate BERT variant (bert-base-uncased, bert-large, etc.)\n",
        "2. **Tokenizer**: Must match the chosen base model\n",
        "3. **Head Architecture**: Design suited to the specific task\n",
        "4. **Dataset**: Minimum 1k samples, recommended 5k+ per class\n",
        "5. **Evaluation Metrics**: Task-appropriate performance measures\n",
        "\n",
        "### Data Preparation\n",
        "\n",
        "1. **Data Cleaning**: Remove duplicates and handle missing values\n",
        "2. **Tokenization**: Apply tokenizer consistent with base model\n",
        "3. **Data Splitting**: Partition into train/validation/test sets (70%/15%/15%)\n",
        "4. **Class Balance**: Verify reasonable distribution across classes\n",
        "5. **PyTorch Formatting**: Create appropriate DataLoader objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0272616",
      "metadata": {
        "id": "c0272616"
      },
      "outputs": [],
      "source": [
        "# Let's create a comprehensive data preparation example\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextClassificationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for text classification with BERT\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Let's create some sample data to demonstrate\n",
        "sample_data = {\n",
        "    'texts': [\n",
        "        \"I absolutely love this product! It's amazing!\",\n",
        "        \"This is the worst thing I've ever bought.\",\n",
        "        \"It's okay, nothing special but does the job.\",\n",
        "        \"Fantastic quality and great customer service!\",\n",
        "        \"Terrible experience, would not recommend.\",\n",
        "        \"Pretty good, meets my expectations.\",\n",
        "        \"Outstanding! Exceeded all my expectations!\",\n",
        "        \"Not worth the money, very disappointed.\",\n",
        "        \"Average product, nothing to complain about.\"\n",
        "    ],\n",
        "    'labels': [2, 0, 1, 2, 0, 1, 2, 0, 1]  # 0=negative, 1=neutral, 2=positive\n",
        "}\n",
        "\n",
        "# Create label mapping for better understanding\n",
        "label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "print(\"Sample Data Overview:\")\n",
        "print(f\"Total samples: {len(sample_data['texts'])}\")\n",
        "\n",
        "# Show class distribution\n",
        "import collections\n",
        "label_counts = collections.Counter(sample_data['labels'])\n",
        "for label, count in label_counts.items():\n",
        "    print(f\"  {label_map[label]}: {count} samples\")\n",
        "\n",
        "# Create dataset and dataloader\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dataset = TextClassificationDataset(\n",
        "    texts=sample_data['texts'],\n",
        "    labels=sample_data['labels'],\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=64\n",
        ")\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "print(\"\\nSample batch from DataLoader:\")\n",
        "for batch in dataloader:\n",
        "    print(f\"Input IDs shape: {batch['input_ids'].shape}\")\n",
        "    print(f\"Attention mask shape: {batch['attention_mask'].shape}\")\n",
        "    print(f\"Labels shape: {batch['label'].shape}\")\n",
        "    print(f\"Sample input IDs: {batch['input_ids'][0][:10]}...\")  # First 10 tokens\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "554df61b",
      "metadata": {
        "id": "554df61b"
      },
      "source": [
        "## 6. Hands-on: Classification Head Replacement <a id=\"sec-classification\"></a>\n",
        "\n",
        "Now let's implement our first head replacement! We'll create a sentiment classification model using BERT with a custom head.\n",
        "\n",
        "### The Plan\n",
        "\n",
        "1. **Load pre-trained BERT** as our base model\n",
        "2. **Create a classification head** (simple linear layer)\n",
        "3. **Freeze BERT weights** (only train the head)\n",
        "4. **Train on our sentiment data**\n",
        "5. **Evaluate performance**\n",
        "\n",
        "This is the most common type of head replacement - perfect for getting started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "619f99c3",
      "metadata": {
        "id": "619f99c3"
      },
      "outputs": [],
      "source": [
        "# Complete implementation of BERT with classification head\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT-based sentiment classifier with replaceable head\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name='bert-base-uncased', num_classes=3, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained BERT\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Classification head - this is what we'll train!\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "        # Freeze BERT parameters\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Initialize the classifier layer\n",
        "        nn.init.normal_(self.classifier.weight, std=0.02)\n",
        "        nn.init.zeros_(self.classifier.bias)\n",
        "\n",
        "        print(f\"Created sentiment classifier:\")\n",
        "        print(f\"Frozen BERT parameters: {sum(p.numel() for p in self.bert.parameters()):,}\")\n",
        "        print(f\"Trainable head parameters: {sum(p.numel() for p in self.classifier.parameters()):,}\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Get BERT outputs\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Use [CLS] token representation\n",
        "        pooled_output = outputs.last_hidden_state[:, 0]  # [batch_size, hidden_size]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        # Apply classification head\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Create our model\n",
        "model = SentimentClassifier(num_classes=3)\n",
        "model = model.to(device)\n",
        "\n",
        "# Create optimizer - only for the head parameters!\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.classifier.parameters(), 'lr': 2e-3}\n",
        "])\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\nModel created and ready for training!\")\n",
        "print(f\"Running on: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53db4576",
      "metadata": {
        "id": "53db4576"
      },
      "outputs": [],
      "source": [
        "# Training function for our head replacement model\n",
        "def train_epoch(model, dataloader, optimizer, loss_fn, device):\n",
        "    \"\"\"Train the model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        # Move batch to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        total_loss += loss.item()\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_model(model, dataloader, loss_fn, device):\n",
        "    \"\"\"Evaluate the model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Let's do a quick training demo\n",
        "print(\"Starting training demonstration...\")\n",
        "\n",
        "# Since we have limited data, let's do just a few epochs\n",
        "num_epochs = 3\n",
        "training_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, dataloader, optimizer, loss_fn, device)\n",
        "\n",
        "    # Evaluate (using same data for demo - normally you'd use separate validation set)\n",
        "    val_loss, val_acc = evaluate_model(model, dataloader, loss_fn, device)\n",
        "\n",
        "    training_history.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_acc': val_acc\n",
        "    })\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeff49ba",
      "metadata": {
        "id": "eeff49ba"
      },
      "outputs": [],
      "source": [
        "# Let's test our trained model with some examples\n",
        "def predict_sentiment(model, tokenizer, text, device, label_map):\n",
        "    \"\"\"Make prediction on a single text\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=64,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "        prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return prediction, probabilities[0].cpu().numpy()\n",
        "\n",
        "# Test with some new examples\n",
        "test_texts = [\n",
        "    \"This movie is absolutely fantastic! I loved every minute of it!\",\n",
        "    \"Boring movie, fell asleep halfway through.\",\n",
        "    \"It was an okay film, not bad but not great either.\",\n",
        "    \"Worst movie ever made, complete waste of time.\",\n",
        "    \"Amazing cinematography and brilliant acting!\"\n",
        "]\n",
        "\n",
        "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "\n",
        "print(\"üß™ Testing our trained sentiment classifier:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, text in enumerate(test_texts):\n",
        "    prediction, probabilities = predict_sentiment(model, tokenizer, text, device, label_map)\n",
        "\n",
        "    print(f\"\\nText {i+1}: '{text[:50]}{'...' if len(text) > 50 else ''}'\")\n",
        "    print(f\"Prediction: {label_map[prediction]}\")\n",
        "    print(f\"Confidence: {probabilities[prediction]:.3f}\")\n",
        "    print(f\"All probabilities: {dict(zip(label_map.values(), probabilities))}\")\n",
        "\n",
        "print(\"\\nHead replacement working successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e2d322c",
      "metadata": {
        "id": "3e2d322c"
      },
      "source": [
        "## 7. Question Answering Head Implementation <a id=\"sec-qa\"></a>\n",
        "\n",
        "Question answering requires predicting the start and end positions of answers within context text, rather than single class labels.\n",
        "\n",
        "### Question Answering Process\n",
        "\n",
        "1. **Input**: Context text and question\n",
        "2. **Processing**: BERT processes concatenated context and question\n",
        "3. **Output**: Start and end token positions of the answer span\n",
        "\n",
        "### QA Head Architecture\n",
        "\n",
        "Question answering heads require two components:\n",
        "- **Start position classifier**: Predicts answer start position for each token\n",
        "- **End position classifier**: Predicts answer end position for each token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee53bb90",
      "metadata": {
        "id": "ee53bb90"
      },
      "outputs": [],
      "source": [
        "class QuestionAnsweringHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Question Answering head that predicts start and end positions\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.qa_outputs = nn.Linear(hidden_size, 2)  # 2 outputs: start_logits, end_logits\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sequence_output: [batch_size, seq_len, hidden_size]\n",
        "        Returns:\n",
        "            start_logits, end_logits: [batch_size, seq_len]\n",
        "        \"\"\"\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.qa_outputs(sequence_output)  # [batch_size, seq_len, 2]\n",
        "\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)  # [batch_size, seq_len]\n",
        "        end_logits = end_logits.squeeze(-1)      # [batch_size, seq_len]\n",
        "\n",
        "        return start_logits, end_logits\n",
        "\n",
        "class BERTQuestionAnswering(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT model with Question Answering head\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained BERT\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "\n",
        "        # QA head\n",
        "        self.qa_head = QuestionAnsweringHead(self.bert.config.hidden_size)\n",
        "\n",
        "        # Freeze BERT (only train the head)\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        print(f\"Created QA model:\")\n",
        "        print(f\"   Frozen BERT parameters: {sum(p.numel() for p in self.bert.parameters()):,}\")\n",
        "        print(f\"   Trainable QA head parameters: {sum(p.numel() for p in self.qa_head.parameters()):,}\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Get BERT outputs for all tokens\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        # Apply QA head\n",
        "        start_logits, end_logits = self.qa_head(sequence_output)\n",
        "\n",
        "        return start_logits, end_logits\n",
        "\n",
        "# Create QA model\n",
        "qa_model = BERTQuestionAnswering()\n",
        "qa_model = qa_model.to(device)\n",
        "\n",
        "print(\"\\nQuestion Answering model ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f304445",
      "metadata": {
        "id": "2f304445"
      },
      "outputs": [],
      "source": [
        "# Let's create a simple QA dataset and demonstrate the concept\n",
        "class QADataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for Question Answering\n",
        "    \"\"\"\n",
        "    def __init__(self, contexts, questions, answers, tokenizer, max_length=256):\n",
        "        self.contexts = contexts\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.contexts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.contexts[idx]\n",
        "        question = self.questions[idx]\n",
        "        answer = self.answers[idx]\n",
        "\n",
        "        # Tokenize context and question together\n",
        "        encoding = self.tokenizer(\n",
        "            question,\n",
        "            context,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Find answer positions in tokenized text\n",
        "        # For simplicity, we'll use a basic approach\n",
        "        start_position = 0  # Simplified - normally you'd find actual positions\n",
        "        end_position = 1    # Simplified - normally you'd find actual positions\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'start_position': torch.tensor(start_position, dtype=torch.long),\n",
        "            'end_position': torch.tensor(end_position, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Sample QA data\n",
        "qa_sample_data = {\n",
        "    'contexts': [\n",
        "        \"Paris is the capital of France. It is located in northern France on the river Seine.\",\n",
        "        \"The Eiffel Tower was built in 1889 for the World's Fair. It stands 324 meters tall.\",\n",
        "        \"Python is a programming language created by Guido van Rossum in 1991.\"\n",
        "    ],\n",
        "    'questions': [\n",
        "        \"What is the capital of France?\",\n",
        "        \"When was the Eiffel Tower built?\",\n",
        "        \"Who created Python?\"\n",
        "    ],\n",
        "    'answers': [\n",
        "        \"Paris\",\n",
        "        \"1889\",\n",
        "        \"Guido van Rossum\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Demonstrate how QA tokenization works\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "print(\"Question Answering Tokenization Example:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "context = qa_sample_data['contexts'][0]\n",
        "question = qa_sample_data['questions'][0]\n",
        "answer = qa_sample_data['answers'][0]\n",
        "\n",
        "print(f\"Context: {context}\")\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")\n",
        "\n",
        "# Tokenize question + context\n",
        "encoding = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    max_length=128,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "# Show the tokenized result\n",
        "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
        "print(f\"\\nTokenized (first 20 tokens): {tokens[:20]}\")\n",
        "print(f\"Total tokens: {len(tokens)}\")\n",
        "\n",
        "# Demonstrate model output\n",
        "with torch.no_grad():\n",
        "    start_logits, end_logits = qa_model(\n",
        "        encoding['input_ids'].to(device),\n",
        "        encoding['attention_mask'].to(device)\n",
        "    )\n",
        "\n",
        "print(f\"\\nModel outputs:\")\n",
        "print(f\"Start logits shape: {start_logits.shape}\")\n",
        "print(f\"End logits shape: {end_logits.shape}\")\n",
        "print(f\"Predicted start position: {torch.argmax(start_logits, dim=1).item()}\")\n",
        "print(f\"Predicted end position: {torch.argmax(end_logits, dim=1).item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1dd86e3",
      "metadata": {
        "id": "e1dd86e3"
      },
      "source": [
        "## 8. Token Classification Head Implementation <a id=\"sec-token-class\"></a>\n",
        "\n",
        "Token classification predicts labels for individual tokens, making it suitable for tasks like Named Entity Recognition (NER) and Part-of-Speech (POS) tagging.\n",
        "\n",
        "### Token Classification Tasks\n",
        "\n",
        "| **Task** | **Function** | **Example** |\n",
        "|----------|--------------|-------------|\n",
        "| **NER** | Identify entities in text | \"John lives in **New York**\" ‚Üí PERSON, CITY |\n",
        "| **POS Tagging** | Classify word types | \"The cat runs\" ‚Üí DET, NOUN, VERB |\n",
        "| **Chunk Detection** | Identify phrases | \"The big red car\" ‚Üí [NP: The big red car] |\n",
        "\n",
        "### Architecture Distinction\n",
        "- **Sequence Classification**: One prediction per sequence (entire text)\n",
        "- **Token Classification**: One prediction per token (individual words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76ce0b01",
      "metadata": {
        "id": "76ce0b01"
      },
      "outputs": [],
      "source": [
        "class TokenClassificationHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Token classification head that predicts a label for each token\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_labels, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sequence_output: [batch_size, seq_len, hidden_size]\n",
        "        Returns:\n",
        "            logits: [batch_size, seq_len, num_labels]\n",
        "        \"\"\"\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "        return logits\n",
        "\n",
        "class BERTTokenClassification(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT model with Token Classification head for NER\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name='bert-base-uncased', num_labels=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained BERT\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "\n",
        "        # Token classification head\n",
        "        self.token_head = TokenClassificationHead(\n",
        "            hidden_size=self.bert.config.hidden_size,\n",
        "            num_labels=num_labels\n",
        "        )\n",
        "\n",
        "        # Freeze BERT (only train the head)\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        print(f\"Created Token Classification model:\")\n",
        "        print(f\"Frozen BERT parameters: {sum(p.numel() for p in self.bert.parameters()):,}\")\n",
        "        print(f\"Trainable head parameters: {sum(p.numel() for p in self.token_head.parameters()):,}\")\n",
        "        print(f\"Number of labels: {num_labels}\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Get BERT outputs for all tokens\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        # Apply token classification head\n",
        "        logits = self.token_head(sequence_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Create token classification model\n",
        "# For this example, let's do simple NER: O (Other), PERSON, LOCATION\n",
        "label_map = {0: \"O\", 1: \"PERSON\", 2: \"LOCATION\"}\n",
        "num_labels = len(label_map)\n",
        "\n",
        "token_model = BERTTokenClassification(num_labels=num_labels)\n",
        "token_model = token_model.to(device)\n",
        "\n",
        "print(f\"\\nToken Classification model ready!\")\n",
        "print(f\"Labels: {label_map}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea8b4637",
      "metadata": {
        "id": "ea8b4637"
      },
      "outputs": [],
      "source": [
        "# Let's demonstrate token classification with a practical example\n",
        "def demonstrate_token_classification():\n",
        "    \"\"\"\n",
        "    Demonstrate how token classification works\n",
        "    \"\"\"\n",
        "    # Sample text with entities\n",
        "    text = \"John Smith lives in New York and works at Google\"\n",
        "\n",
        "    print(\"Token Classification Demonstration:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Text: {text}\")\n",
        "\n",
        "    # Tokenize\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=64,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Get tokens for display\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
        "\n",
        "    # Get model predictions\n",
        "    token_model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = token_model(\n",
        "            encoding['input_ids'].to(device),\n",
        "            encoding['attention_mask'].to(device)\n",
        "        )\n",
        "\n",
        "        # Get predictions for each token\n",
        "        predictions = torch.argmax(logits, dim=-1)[0]  # [seq_len]\n",
        "\n",
        "    print(f\"\\nToken-by-token predictions:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Show first 15 meaningful tokens (skip padding)\n",
        "    for i, (token, pred) in enumerate(zip(tokens[:15], predictions[:15])):\n",
        "        if token not in ['[PAD]', '[CLS]', '[SEP]']:\n",
        "            label = label_map[pred.item()]\n",
        "            print(f\"Token: '{token:12}' ‚Üí Label: {label}\")\n",
        "\n",
        "    # Create a visualization\n",
        "    meaningful_tokens = []\n",
        "    meaningful_labels = []\n",
        "\n",
        "    for token, pred in zip(tokens, predictions):\n",
        "        if token not in ['[PAD]', '[CLS]', '[SEP]']:\n",
        "            meaningful_tokens.append(token)\n",
        "            meaningful_labels.append(label_map[pred.item()])\n",
        "\n",
        "    print(f\"\\nVisualization:\")\n",
        "    print(\"=\" * 50)\n",
        "    for token, label in zip(meaningful_tokens, meaningful_labels):\n",
        "        if label != \"O\":\n",
        "            print(f\"'{token}' ‚Üí {label}\")\n",
        "        else:\n",
        "            print(f\"'{token}' ‚Üí _\")\n",
        "\n",
        "demonstrate_token_classification()\n",
        "\n",
        "# Show the difference in output shapes between different tasks\n",
        "print(f\"\\nComparing Output Shapes:\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"Text Classification: [batch_size, num_classes] = [1, 3]\")\n",
        "print(f\"Question Answering: [batch_size, seq_len] (start & end) = [1, 64] each\")\n",
        "print(f\"Token Classification: [batch_size, seq_len, num_labels] = [1, 64, 3]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2e7a7ad",
      "metadata": {
        "id": "e2e7a7ad"
      },
      "source": [
        "## 9. Performance Comparison & Best Practices <a id=\"sec-best-practices\"></a>\n",
        "\n",
        "### Performance Considerations\n",
        "\n",
        "| **Metric** | **Head Replacement** | **Full Fine-tuning** | **Feature Extraction** |\n",
        "|------------|---------------------|---------------------|------------------------|\n",
        "| **Training Time** | Fast (minutes) | Slow (hours) | Very Fast (seconds) |\n",
        "| **Data Efficiency** | Good (1k+ samples) | Requires more (10k+) | Works with small data |\n",
        "| **Performance** | Good-Very Good | Best | Lower |\n",
        "| **Memory Usage** | Moderate | High | Low |\n",
        "| **Multiple Tasks** | Easy | Requires separate models | Very Easy |\n",
        "\n",
        "### Best Practices for Head Replacement\n",
        "\n",
        "#### Data Preparation\n",
        "- **Quality over quantity**: Prioritize high-quality samples over large volumes\n",
        "- **Balanced datasets**: Maintain reasonable class distribution\n",
        "- **Proper validation**: Use separate validation sets\n",
        "- **Text preprocessing**: Clean data without over-processing\n",
        "\n",
        "#### Model Architecture\n",
        "- **Start simple**: Begin with single linear layer\n",
        "- **Add complexity gradually**: Introduce dropout, multiple layers, skip connections as needed\n",
        "- **Monitor overfitting**: Smaller heads reduce overfitting risk\n",
        "\n",
        "#### Training Strategy\n",
        "- **Learning rates**: Use higher rates (1e-3 to 1e-2) for head training\n",
        "- **Training duration**: 3-10 epochs typically sufficient\n",
        "- **Early stopping**: Monitor validation loss\n",
        "- **Warm-up**: Optional for training stability"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff949176",
      "metadata": {
        "id": "ff949176"
      },
      "source": [
        "## 10. Practice Exercise: Movie Review Sentiment Analysis <a id=\"sec-exercises\"></a>\n",
        "\n",
        "### Exercise Overview\n",
        "\n",
        "**Task**: Build a BERT-based classifier for movie review sentiment analysis\n",
        "\n",
        "**Objective**: Classify reviews as \"Positive\", \"Negative\", or \"Neutral\"\n",
        "\n",
        "**Implementation**: Complete pipeline from data preparation to evaluation\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Create custom datasets for BERT\n",
        "- Build and train classification heads\n",
        "- Handle real-world text data\n",
        "- Evaluate model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d24aaada",
      "metadata": {
        "id": "d24aaada"
      },
      "source": [
        "### Step 1: Prepare the Movie Review Dataset\n",
        "\n",
        "First, let's create a realistic movie review dataset. Your task is to complete the missing parts!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbbbafbe",
      "metadata": {
        "id": "fbbbafbe"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create Movie Review Dataset\n",
        "# TODO: Complete the missing parts marked with \"# YOUR CODE HERE\"\n",
        "\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample movie review data\n",
        "movie_reviews = {\n",
        "    'reviews': [\n",
        "        \"This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.\",\n",
        "        \"Terrible film. Boring story, bad acting, waste of time. Would not recommend to anyone.\",\n",
        "        \"It was okay, nothing special. Some good moments but overall just average entertainment.\",\n",
        "        \"One of the best movies I've ever seen! Brilliant cinematography and outstanding performances.\",\n",
        "        \"Not great, not terrible. Watchable but forgettable. Could have been much better.\",\n",
        "        \"Awful movie with terrible dialogue. The worst film of the year without any doubt.\",\n",
        "        \"Decent film with good character development. Worth watching but not a masterpiece.\",\n",
        "        \"Absolutely loved it! Amazing story, great acting, and beautiful visuals. Highly recommended!\",\n",
        "        \"Mediocre at best. Some interesting ideas but poor execution. Left me disappointed.\",\n",
        "        \"Outstanding masterpiece! Every scene was perfect. This will be remembered as a classic.\",\n",
        "        \"Boring and predictable. Nothing new or exciting. Felt like a waste of money.\",\n",
        "        \"Pretty good movie overall. Well-made with solid performances from the entire cast.\"\n",
        "    ],\n",
        "    'labels': [2, 0, 1, 2, 1, 0, 1, 2, 1, 2, 0, 1]  # 0=Negative, 1=Neutral, 2=Positive\n",
        "}\n",
        "\n",
        "# Create label mapping\n",
        "label_to_text = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "print(\"Movie Review Dataset Created!\")\n",
        "print(f\"Total reviews: {len(movie_reviews['reviews'])}\")\n",
        "\n",
        "# TODO: Calculate and print the class distribution\n",
        "# HINT: Use collections.Counter on movie_reviews['labels']\n",
        "import collections\n",
        "class_distribution = # YOUR CODE HERE\n",
        "print(\"\\nClass Distribution:\")\n",
        "for label_id, count in class_distribution.items():\n",
        "    label_name = # YOUR CODE HERE  # Get label name from label_to_text\n",
        "    print(f\"  {label_name}: {count} reviews\")\n",
        "\n",
        "# TODO: Split the data into train and validation sets\n",
        "# HINT: Use train_test_split with test_size=0.3 and random_state=42\n",
        "X_train, X_val, y_train, y_val = # YOUR CODE HERE\n",
        "\n",
        "print(f\"\\nData Split:\")\n",
        "print(f\"  Training samples: {len(X_train)}\")\n",
        "print(f\"  Validation samples: {len(X_val)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba91a670",
      "metadata": {
        "id": "ba91a670"
      },
      "source": [
        "### Step 2: Create the BERT Movie Classifier\n",
        "\n",
        "Now build your BERT-based movie review classifier. Complete the missing methods!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccde0e70",
      "metadata": {
        "id": "ccde0e70"
      },
      "outputs": [],
      "source": [
        "# Step 2: Build BERT Movie Review Classifier\n",
        "# TODO: Complete the missing parts in the class definition\n",
        "\n",
        "class MovieReviewClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT-based movie review sentiment classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name='bert-base-uncased', num_classes=3, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained BERT\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "\n",
        "        # TODO: Add dropout layer\n",
        "        self.dropout = # YOUR CODE HERE\n",
        "\n",
        "        # TODO: Create classification head (Linear layer)\n",
        "        # HINT: Input size should be self.bert.config.hidden_size, output size is num_classes\n",
        "        self.classifier = # YOUR CODE HERE\n",
        "\n",
        "        # TODO: Freeze BERT parameters\n",
        "        # HINT: Loop through self.bert.parameters() and set requires_grad = False\n",
        "        for param in self.bert.parameters():\n",
        "            # YOUR CODE HERE\n",
        "\n",
        "        print(f\"Created Movie Review Classifier:\")\n",
        "        print(f\"   Frozen BERT parameters: {sum(p.numel() for p in self.bert.parameters()):,}\")\n",
        "        print(f\"   Trainable parameters: {sum(p.numel() for p in self.classifier.parameters()):,}\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # TODO: Get BERT outputs\n",
        "        outputs = # YOUR CODE HERE\n",
        "\n",
        "        # TODO: Get [CLS] token representation (first token)\n",
        "        cls_output = # YOUR CODE HERE  # Shape: [batch_size, hidden_size]\n",
        "\n",
        "        # TODO: Apply dropout\n",
        "        cls_output = # YOUR CODE HERE\n",
        "\n",
        "        # TODO: Apply classification head\n",
        "        logits = # YOUR CODE HERE\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Create the model\n",
        "print(\"üé¨ Creating Movie Review Classifier...\")\n",
        "movie_classifier = MovieReviewClassifier(num_classes=3)\n",
        "movie_classifier = movie_classifier.to(device)\n",
        "print(\"Model created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02fb472b",
      "metadata": {
        "id": "02fb472b"
      },
      "source": [
        "### Step 3: Prepare Data for Training\n",
        "\n",
        "Create PyTorch datasets and data loaders. Fill in the missing dataset implementation!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b06924c9",
      "metadata": {
        "id": "b06924c9"
      },
      "outputs": [],
      "source": [
        "# Step 3: Create Dataset and DataLoaders\n",
        "# TODO: Complete the MovieReviewDataset class\n",
        "\n",
        "class MovieReviewDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for movie review classification\n",
        "    \"\"\"\n",
        "    def __init__(self, reviews, labels, tokenizer, max_length=128):\n",
        "        self.reviews = reviews\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = str(self.reviews[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # TODO: Tokenize the review\n",
        "        # HINT: Use self.tokenizer with truncation=True, padding='max_length', return_tensors='pt'\n",
        "        encoding = # YOUR CODE HERE\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "print(\"Creating datasets...\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# TODO: Create training and validation datasets\n",
        "train_dataset = # YOUR CODE HERE\n",
        "val_dataset = # YOUR CODE HERE\n",
        "\n",
        "# TODO: Create data loaders with batch_size=4\n",
        "train_loader = # YOUR CODE HERE\n",
        "val_loader = # YOUR CODE HERE\n",
        "\n",
        "print(f\"Datasets created!\")\n",
        "print(f\"   Training batches: {len(train_loader)}\")\n",
        "print(f\"   Validation batches: {len(val_loader)}\")\n",
        "\n",
        "# Test the data loader\n",
        "print(\"\\n Sample batch:\")\n",
        "for batch in train_loader:\n",
        "    print(f\"  Input IDs shape: {batch['input_ids'].shape}\")\n",
        "    print(f\"  Attention mask shape: {batch['attention_mask'].shape}\")\n",
        "    print(f\"  Labels shape: {batch['label'].shape}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e0d0f04",
      "metadata": {
        "id": "6e0d0f04"
      },
      "source": [
        "### Step 4: Train Your Model\n",
        "\n",
        "Implement the training loop. Complete the missing training logic!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6009370",
      "metadata": {
        "id": "e6009370"
      },
      "outputs": [],
      "source": [
        "# Step 4: Training Setup and Loop\n",
        "# TODO: Complete the training implementation\n",
        "\n",
        "# TODO: Create optimizer for only the classifier parameters\n",
        "# HINT: Use Adam with learning rate 2e-3\n",
        "optimizer = # YOUR CODE HERE\n",
        "\n",
        "# TODO: Create loss function\n",
        "# HINT: Use CrossEntropyLoss\n",
        "loss_fn = # YOUR CODE HERE\n",
        "\n",
        "def train_movie_classifier(model, train_loader, val_loader, optimizer, loss_fn, num_epochs=3):\n",
        "    \"\"\"\n",
        "    Train the movie review classifier\n",
        "    \"\"\"\n",
        "    training_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nüöÇ Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            # Move to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # TODO: Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            logits = # YOUR CODE HERE\n",
        "\n",
        "            # TODO: Calculate loss\n",
        "            loss = # YOUR CODE HERE\n",
        "\n",
        "            # TODO: Backward pass\n",
        "            # YOUR CODE HERE  # loss.backward()\n",
        "            # YOUR CODE HERE  # optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            train_loss += loss.item()\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            train_correct += (predictions == labels).sum().item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                # TODO: Forward pass (no gradients needed)\n",
        "                logits = # YOUR CODE HERE\n",
        "                loss = loss_fn(logits, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                val_correct += (predictions == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_acc = train_correct / train_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        epoch_stats = {\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss / len(train_loader),\n",
        "            'train_acc': train_acc,\n",
        "            'val_loss': val_loss / len(val_loader),\n",
        "            'val_acc': val_acc\n",
        "        }\n",
        "\n",
        "        training_history.append(epoch_stats)\n",
        "\n",
        "        print(f\"Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}\")\n",
        "        print(f\"Train Loss: {epoch_stats['train_loss']:.3f}, Val Loss: {epoch_stats['val_loss']:.3f}\")\n",
        "\n",
        "    return training_history\n",
        "\n",
        "# TODO: Start training!\n",
        "print(\"üöÄ Starting training...\")\n",
        "# YOUR CODE HERE  # Call train_movie_classifier function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fe85b8a",
      "metadata": {
        "id": "7fe85b8a"
      },
      "source": [
        "### Step 5: Test Your Trained Model\n",
        "\n",
        "Finally, test your model on new movie reviews! Complete the prediction function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a49f7d5e",
      "metadata": {
        "id": "a49f7d5e"
      },
      "outputs": [],
      "source": [
        "# Step 5: Test the Trained Model\n",
        "# TODO: Complete the prediction function\n",
        "\n",
        "def predict_movie_sentiment(model, tokenizer, review_text, device):\n",
        "    \"\"\"\n",
        "    Predict sentiment for a single movie review\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # TODO: Tokenize the review\n",
        "    encoding = # YOUR CODE HERE  # Use tokenizer with appropriate parameters\n",
        "\n",
        "    # Move to device\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # TODO: Get model predictions\n",
        "        logits = # YOUR CODE HERE\n",
        "\n",
        "        # TODO: Convert to probabilities using softmax\n",
        "        probabilities = # YOUR CODE HERE\n",
        "\n",
        "        # TODO: Get predicted class\n",
        "        prediction = # YOUR CODE HERE  # Use torch.argmax\n",
        "\n",
        "    return prediction.item(), probabilities.cpu().numpy()\n",
        "\n",
        "# Test on new movie reviews\n",
        "test_reviews = [\n",
        "    \"This movie was a complete masterpiece! Incredible acting and stunning visuals.\",\n",
        "    \"Boring and poorly written. Complete waste of time and money.\",\n",
        "    \"It was okay, not bad but nothing special either.\",\n",
        "    \"Absolutely terrible! Worst movie I've ever watched in my entire life.\",\n",
        "    \"Pretty good film with some great moments and solid performances.\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing Model on New Reviews:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, review in enumerate(test_reviews):\n",
        "    # TODO: Get prediction for each review\n",
        "    pred_label, probabilities = # YOUR CODE HERE\n",
        "\n",
        "    predicted_sentiment = label_to_text[pred_label]\n",
        "    confidence = probabilities[pred_label]\n",
        "\n",
        "    print(f\"\\nüìù Review {i+1}: '{review[:50]}{'...' if len(review) > 50 else ''}'\")\n",
        "    print(f\"üéØ Prediction: {predicted_sentiment}\")\n",
        "    print(f\"üìä Confidence: {confidence:.3f}\")\n",
        "\n",
        "    # Show all probabilities\n",
        "    all_probs = {label_to_text[j]: prob for j, prob in enumerate(probabilities)}\n",
        "    print(f\"üìà All probabilities: {all_probs}\")\n",
        "\n",
        "print(\"\\nüéâ Congratulations! You've successfully completed the BERT head replacement exercise!\")\n",
        "print(\"üí° Try experimenting with:\")\n",
        "print(\"   ‚Ä¢ Different dropout values\")\n",
        "print(\"   ‚Ä¢ More training epochs\")\n",
        "print(\"   ‚Ä¢ Different learning rates\")\n",
        "print(\"   ‚Ä¢ Adding more layers to the head\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdbbd1d5",
      "metadata": {
        "id": "bdbbd1d5"
      },
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Core Concepts\n",
        "- **BERT Architecture**: Understanding how BERT creates text representations\n",
        "- **Head Replacement**: Replacing task-specific layers while preserving base knowledge\n",
        "- **Base vs Head**: Conceptual separation enabling effective transfer learning\n",
        "\n",
        "### Practical Skills\n",
        "- **Text Classification**: Sentiment analysis with frozen BERT and trainable head\n",
        "- **Question Answering**: Locating answer spans in context text\n",
        "- **Token Classification**: Individual word labeling for NER tasks\n",
        "- **Advanced Techniques**: Attention pooling, weighted loss, multi-layer heads\n",
        "\n",
        "### When to Use Head Replacement\n",
        "- Small to medium datasets (1k-10k samples)\n",
        "- Quick prototyping and experimentation\n",
        "- Multiple related tasks with shared base model\n",
        "- Resource-constrained environments\n",
        "- Preserving general language knowledge\n",
        "\n",
        "### Performance Guidelines\n",
        "- **Start simple**: Single linear layer often sufficient\n",
        "- **Handle imbalanced data**: Apply weighted loss functions  \n",
        "- **Experiment with pooling**: CLS, mean, max, or attention-based approaches\n",
        "- **Monitor training**: Use proper validation and early stopping\n",
        "- **Scale gradually**: Add complexity only when necessary\n",
        "\n",
        "### Next Steps\n",
        "- Experiment with different BERT variants (RoBERTa, DeBERTa)\n",
        "- Explore domain-specific pre-trained models\n",
        "- Investigate multi-task learning with shared base models\n",
        "- Compare head replacement vs full fine-tuning performance\n",
        "- Develop production-ready inference pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3b315fb",
      "metadata": {
        "id": "c3b315fb"
      },
      "source": [
        "## Additional Resources\n",
        "\n",
        "### Essential Reading\n",
        "- [BERT Paper (Original)](https://arxiv.org/abs/1810.04805) - The foundational paper\n",
        "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers) - Comprehensive guide\n",
        "- [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/) - Visual explanations\n",
        "\n",
        "### Advanced Topics\n",
        "- [RoBERTa Improvements](https://arxiv.org/abs/1907.11692) - Better training strategies\n",
        "- [DeBERTa Enhancements](https://arxiv.org/abs/2006.03654) - Disentangled attention\n",
        "- [Domain Adaptation with BERT](https://arxiv.org/abs/2004.02288) - Specialized domains"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
