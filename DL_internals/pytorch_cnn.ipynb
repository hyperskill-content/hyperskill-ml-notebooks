{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8a6d3a",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs) with PyTorch\n",
    "Welcome! In this notebook you will learn how convolutional neural networks work and how to implement them in PyTorch. Throughout the notebook, theory cells explain the core concepts, code cells demonstrate them, and **exercises** help you practice.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hyperskill-content/hyperskill-ml-notebooks/blob/main/DL_internals/pytorch_tensors.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f23968c",
   "metadata": {},
   "source": [
    "## üöÄ Prerequisites <a id=\"prerequisites\"></a>\n",
    "\n",
    "Make sure you're comfortable with the topics below before starting this notebook:\n",
    "\n",
    "| # | Topic (clickable links)|\n",
    "|---|-------|\n",
    "| 1 | **[Convolutions](https://hyperskill.org/learn/step/39250)**  |\n",
    "| 2 | **[Pooling](https://hyperskill.org/learn/step/39100)** |\n",
    "| 3 | **[Padding](https://hyperskill.org/learn/step/39261)** | \n",
    "| 4 | **[Batch normalization](https://hyperskill.org/learn/step/41705)** | \n",
    "| 5 | **[Default train-validation loop](https://hyperskill.org/learn/step/42852)** | \n",
    "| 6 | **[Adam optimizer](https://hyperskill.org/learn/step/35555)** |\n",
    "| 7 | **[Activation functions](https://hyperskill.org/learn/step/35741)** |\n",
    "| 8 | **[Visualization of convolutions](https://animatedai.github.io)** | \n",
    "\n",
    "*If you're new to any item above, review it quickly, then dive back in here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c377fd",
   "metadata": {},
   "source": [
    "# üìë Table of Contents  \n",
    "  \n",
    "1. [What‚Äôs the big idea?](#sec-idea)  \n",
    "2. [Convolution Layer in Plain Words](#sec-conv)  \n",
    "3. [Receptive Field ‚Äî How far can a neuron ‚Äúsee‚Äù?](#sec-rf)  \n",
    "4. [Activation Function](#4-activation-function)  \n",
    "5. [Pooling](#sec-pool)  \n",
    "6. [Putting Blocks Together ‚Äî A Mini CNN](#6-putting-blocks-together--a-mini-cnn)  \n",
    "7. [Pre-trained Models & Architecture Zoo](#sec-pretrained)  \n",
    "8. [Training Loop ‚Äî Putting the CNN to Work](#sec-train-loop)  \n",
    "9. [1 √ó 1 Convolutions ‚Äî Tiny Filters, Big Impact](#sec-1x1)  \n",
    "10. [Practice Section](#sec-exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2358a4",
   "metadata": {},
   "source": [
    "## 1. What's the big idea? <a id=\"sec-idea\"></a>\n",
    "\n",
    "Imagine you're trying to recognize your friend's face in a photo. A regular neural network (called **Fully Connected Neural Network** or **FCNN**) would look at EVERY single pixel in the entire image at once - like trying to memorize a phone book by reading all numbers simultaneously!\n",
    "\n",
    "That's pretty crazy, right? Your brain doesn't work this way. When you look at a face, you first notice eyes, then nose, then mouth - **piece by piece**.\n",
    "\n",
    "**CNNs are smarter than FCNNs at image understanding**:\n",
    "\n",
    "| **FCNN** (Old way) | **CNN** (Smart way) |\n",
    "|-------------------|-------------------|\n",
    "| Looks at ALL pixels at once  | Looks at small patches (like 3√ó3)  |\n",
    "| Each neuron connects to EVERY pixel | Same filter slides over the whole image |\n",
    "| Millions of connections = slow  | Fewer connections = fast & efficient  |\n",
    "| Doesn't understand \"spatial relationships\" | Knows that nearby pixels are related |\n",
    "\n",
    "When we feed an image into a FCNN, we need to flatten the pixels into a 1D array, which causes the loss of spatial information. A **Convolutional Neural Network (CNN)** is basically a smart way to look at\n",
    "photos (and even audio) **piece by piece** instead of all at once.\n",
    "\n",
    "* **Local view** ‚Äì filters only look at a small patch (e.g. 3√ó3)\n",
    "* **Weight sharing** ‚Äì the same filter slides over the whole image  \n",
    "* **Stacked layers** ‚Äì early layers find edges (high level features), later layers find more complex patterns (low level features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f87b3",
   "metadata": {},
   "source": [
    "## 2. Convolution Layer in Plain Words <a id=\"sec-conv\"></a>\n",
    "\n",
    "Think of a filter as a sliding window:\n",
    "\n",
    "* You place it on a 3√ó3 patch of pixels.  \n",
    "* Multiply numbers element-wise, add them up ‚Üí **one output pixel**.  \n",
    "* Slide right by **stride `S`** steps, repeat.\n",
    "\n",
    "**Padding** `P` adds a border of zeros so the sliding window can fully cover edge pixels without going out of bounds.\n",
    "\n",
    "Formula for output size (1-D for simplicity):\n",
    "\n",
    "$$\n",
    "\\text{output\\_length} = \\frac{N + 2P - F}{S} + 1\n",
    "$$\n",
    "\n",
    "Where  \n",
    "\n",
    "* `N` = input size (e.g., 32)  \n",
    "* `F` = filter size (e.g., 3)  \n",
    "* `S` = stride  \n",
    "* `P` = padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd592bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîé Code peek ‚Äî one conv layer\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=8,\n",
    "                 kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "x = torch.randn(1, 3, 32, 32)  # fake RGB image\n",
    "out = conv(x)\n",
    "print(\"input :\", x.shape)\n",
    "print(\"output:\", out.shape)   # (1, 8, 32, 32) ‚Äî same H/W thanks to P=1 (same padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d768ea6",
   "metadata": {},
   "source": [
    "## 3. Receptive Field ‚Äî How far can a neuron \"see\"? <a id=\"sec-rf\"></a>\n",
    "\n",
    "Add more conv layers ‚Üí each output pixel depends on **a larger area**\n",
    "of the original image.\n",
    "\n",
    "* Layer 1 (3√ó3 filter) ‚Üí sees 3√ó3 pixels  \n",
    "* Layer 2 ‚Üí sees 5√ó5 (because 3 grows to 5)‚Ä¶  \n",
    "* Keep stacking and the receptive field grows like gossip in a small town.\n",
    "\n",
    "> **Why stack two 3√ó3 filters instead of one 5√ó5?** (this is pretty common question on ML interviews)\n",
    "\n",
    "* **Fewer weights** ‚Äì a single 5√ó5 filter has **25 parameters**, while two\n",
    "  consecutive 3√ó3 filters use **9 + 9 = 18**.  \n",
    "  That's 28 % less memory and math.\n",
    "\n",
    "* **More non-linearity** ‚Äì you get **two ReLU activations** instead of one,\n",
    "  letting the network learn richer functions for (almost) the same receptive\n",
    "  field.\n",
    "\n",
    "* **Same receptive field** ‚Äì two 3√ó3 layers (no padding tricks) end up \"seeing\"\n",
    "  a 5√ó5 area of the input, so you don't lose context.\n",
    "\n",
    "It's like using two small magnifying glasses üîçüîç: you cover the same area as a\n",
    "big lens but carry less weight and get an extra layer of detail.\n",
    "\n",
    "![Expanding receptive field through stacked convolutional layers](https://www.jeremyjordan.me/content/images/2018/04/Screen-Shot-2018-04-17-at-5.32.45-PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5b698",
   "metadata": {},
   "source": [
    "## 4. Activation Function <a id=\"4-activation-function\"></a>\n",
    "\n",
    "After the convolution, we pass numbers through activation function, the most common one is **ReLU**:\n",
    "\n",
    "$$\\operatorname{ReLU}(x) = \\max(0,\\; x)$$\n",
    "\n",
    "Why? It keeps positive signals and kills the boring negative ones ‚Üí helps the\n",
    "network learn non-linear stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb66bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "print(relu(torch.tensor([-2., 0., 3.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b659a2",
   "metadata": {},
   "source": [
    "**‚ùì What is a CNN (or any neural network) without activations?**\n",
    "\n",
    "A CNN (or any neural network) without activation functions is just a stack of linear operations‚Äîessentially, a single big linear transformation. Without non-linear activations like ReLU, the network cannot learn or represent complex, non-linear patterns. It would be no more powerful than a single linear layer, no matter how many layers you stack. Non-linear activations are what give neural networks their expressive power!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a882d5",
   "metadata": {},
   "source": [
    "## 5. Pooling <a id=\"sec-pool\"></a>\n",
    "\n",
    "**Max-pooling** keeps the *strongest* signal in a window (e.g., 2√ó2).\n",
    "This:\n",
    "\n",
    "* Shrinks the feature map (less memory)  \n",
    "* Adds a bit of translation tolerance (small shifts, same max)\n",
    "\n",
    "Common choice: `kernel_size=2`, `stride=2` halves H and W.\n",
    "\n",
    "![Illustration of 2√ó2 max pooling](https://production-media.paperswithcode.com/methods/MaxpoolSample2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4441bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "feat = torch.arange(0,16).view(1,1,4,4).float()\n",
    "print(\"before:\\n\", feat.squeeze())\n",
    "print(\"after pool:\\n\", pool(feat).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ae1fc",
   "metadata": {},
   "source": [
    "## 6. Putting Blocks Together ‚Äî A Mini CNN <a id=\"6-putting-blocks-together--a-mini-cnn\"></a>\n",
    "\n",
    "Conv ‚Üí ReLU ‚Üí Pool ‚Üí Conv ‚Üí ReLU ‚Üí Pool ‚Üí Flatten ‚Üí Linear ‚Üí **Logits**\n",
    "\n",
    "Below, we build a tiny network for 10-class images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                # 32‚Üí16\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                 # 16‚Üí8\n",
    "        )\n",
    "        self.classifier = nn.Linear(32*8*8, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)   # flatten\n",
    "        return self.classifier(x)\n",
    "\n",
    "net = TinyCNN()\n",
    "fake = torch.randn(4, 3, 32, 32)\n",
    "print(net(fake).shape)   # (4, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2ef05",
   "metadata": {},
   "source": [
    "## 7. Pre-trained Models & Architecture Zoo <a id=\"sec-pretrained\"></a>\n",
    "\n",
    "> üîë **Key idea:** in ~99 % of real projects you **don't design a CNN from scratch**.  \n",
    "> You download a ready-made network that was trained on ImageNet (or similar) and\n",
    "> either **use it as-is** or **fine-tune** it for your task.\n",
    "\n",
    "### Why use pre-trained networks?\n",
    "\n",
    "| Benefit | What it means |\n",
    "|---------|---------------|\n",
    "| **Speed** | Skip days (or weeks) of training üöÄ |\n",
    "| **Accuracy** | Years of research baked in (ResNet, EfficientNet, ViT‚Ä¶) |\n",
    "| **Data-efficient** | Works even if you only have a few hundred labelled images |\n",
    "| **Less tuning** | Good defaults for learning-rate, normalisation, etc. |\n",
    "\n",
    "Popular families:\n",
    "\n",
    "* **[VGG](https://hyperskill.org/learn/step/39368)** ‚Äì simple, but large  \n",
    "* **[ResNet](https://hyperskill.org/learn/step/40068)** ‚Äì skip connections üèÉ‚Äç‚ôÇÔ∏è (good baseline)  \n",
    "* **[EfficientNet](https://hyperskill.org/learn/step/43876)** ‚Äì better accuracy / size trade-off  \n",
    "* **Vision Transformers (ViT, Swin)** ‚Äì attention instead of conv layers\n",
    "\n",
    "### Typical workflow\n",
    "\n",
    "1. **Load** a pre-trained backbone.  \n",
    "2. **Freeze** most layers (optional).  \n",
    "3. **Replace** the final classifier with one matching your number of classes.  \n",
    "4. Train a few epochs on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ba11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "# Load ResNet-18 pre-trained on ImageNet\n",
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Freeze all layers (optional; comment out to fine-tune)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Replace the final layer (ResNet-18 has 512-dim features)\n",
    "num_classes = 3          # e.g. cat, dog, llama\n",
    "model.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "# Ready for your training loop\n",
    "dummy = torch.randn(2, 3, 224, 224)\n",
    "print(\"logits shape:\", model(dummy).shape)   # (2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8fa71",
   "metadata": {},
   "source": [
    "### When should you build a custom model?\n",
    "\n",
    "Only when:\n",
    "\n",
    "* Your input is **radically different** (e.g. 3-D medical scans).  \n",
    "* You need a **tiny** network for microcontrollers.  \n",
    "* You're doing research and inventing a new architecture.\n",
    "\n",
    "For almost everything else, grab a pre-trained model, tweak, and ship. üö¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b7610",
   "metadata": {},
   "source": [
    "## 8. Training Loop ‚Äî Putting the CNN to Work <a id=\"sec-train-loop\"></a>\n",
    "\n",
    "So far we've built models and passed dummy data through them.  \n",
    "Now let's **train** one for real (well, for a single epoch).\n",
    "\n",
    "Key pieces:\n",
    "\n",
    "| Piece | PyTorch object | What it does |\n",
    "|-------|----------------|--------------|\n",
    "| **Dataset** | `torchvision.datasets.*` | knows how to load images + labels |\n",
    "| **DataLoader** | `torch.utils.data.DataLoader` | serves mini-batches |\n",
    "| **Model** | our `TinyCNN` (or any pre-trained net) | predicts logits |\n",
    "| **Loss** | `nn.CrossEntropyLoss` | compares logits & labels |\n",
    "| **Optimizer** | `torch.optim.SGD / Adam` | updates weights |\n",
    "\n",
    "We'll use **CIFAR-10** (tiny 32√ó32 colour images, 10 classes).  \n",
    "Feel free to swap in Fashion-MNIST if your internet is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)\n",
    "\n",
    "# 1. Data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                   # (0..1)\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))  # to [-1,1]\n",
    "])\n",
    "\n",
    "train_set = datasets.CIFAR10(root=\"./data\", train=True,\n",
    "                             download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "# 2. Model\n",
    "model = TinyCNN().to(device)\n",
    "\n",
    "# 3. Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 4. Training loop (1 epoch demo)\n",
    "for images, labels in train_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    # forward\n",
    "    logits = model(images)\n",
    "    loss   = criterion(logits, labels)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Finished 1 epoch!  Final mini-batch loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15fbbfc",
   "metadata": {},
   "source": [
    "### What's happening each iteration?\n",
    "\n",
    "1. **Forward pass** ‚Äì model predicts logits from images.  \n",
    "2. **Loss calculation** ‚Äì `CrossEntropyLoss` compares logits vs. true labels.  \n",
    "3. **Zero grads** ‚Äì clear old gradients (`optimizer.zero_grad()`).  \n",
    "4. **Backward pass** ‚Äì `loss.backward()` fills `.grad` for each weight.  \n",
    "5. **Optimizer step** ‚Äì `optimizer.step()` nudges weights to reduce loss.\n",
    "\n",
    "Run more epochs, tweak `lr`, or switch to `Adam` and watch accuracy rise.  \n",
    "When loss plateaus, you're ready to save the model:\n",
    "\n",
    "```python\n",
    "torch.save(model.state_dict(), \"tinycnn_cifar10.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f927950",
   "metadata": {},
   "source": [
    "## 9. 1 √ó 1 Convolutions ‚Äî Tiny Filters, Big Impact <a id=\"sec-1x1\"></a>\n",
    "\n",
    "A **1 √ó 1 convolution** may look useless (it covers only one pixel!),  \n",
    "but it's a secret weapon in many famous architectures (ResNet,\n",
    "MobileNet, EfficientNet).\n",
    "\n",
    "### What does a 1 √ó 1 filter actually do?\n",
    "\n",
    "* Works **across channels**, not across height/width.  \n",
    "  *Think of it as a mini fully-connected layer for each pixel location.*\n",
    "\n",
    "* **Mixes features** ‚Äî combines information from all input channels and\n",
    "  outputs a new set of channels.\n",
    "\n",
    "* **Shrinks or expands depth** ‚Äî great for **bottlenecks** to cut parameter\n",
    "  count before a bigger 3 √ó 3 layer.\n",
    "\n",
    "| Example | In-channels ‚Üí Out-channels | Parameters |\n",
    "|---------|---------------------------|------------|\n",
    "| 3√ó3 conv | 64 ‚Üí 64 | 64 √ó 64 √ó 3 √ó 3 = 36 864 |\n",
    "| **1√ó1 + 3√ó3 combo** | 64 ‚Üí **16** (1√ó1), then 16 ‚Üí 64 (3√ó3) | 64 √ó 16 √ó 1 √ó 1 + 16 √ó 64 √ó 3 √ó 3 = **10 240** |\n",
    "\n",
    "‚Üí **72 % fewer weights** for the same spatial \"receptive field\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03583ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "\n",
    "bottleneck = nn.Sequential(\n",
    "    nn.Conv2d(64, 16, kernel_size=1), nn.ReLU(),   # 1√ó1 reduce\n",
    "    nn.Conv2d(16, 64, kernel_size=3, padding=1), nn.ReLU()  # 3√ó3 process\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 64, 32, 32)\n",
    "print(\"input shape :\", x.shape)\n",
    "y = bottleneck(x)\n",
    "print(\"output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b07044",
   "metadata": {},
   "source": [
    "### Practical uses\n",
    "\n",
    "* **Inception modules** ‚Äì mix 1 √ó 1, 3 √ó 3, 5 √ó 5 in parallel.\n",
    "* **ResNet bottleneck blocks** ‚Äì 1 √ó 1 reduce ‚Üí 3 √ó 3 ‚Üí 1 √ó 1 expand.\n",
    "* **MobileNet and EfficientNet blocks** ‚Äì \"inverted\" bottlenecks with depthwise + 1 √ó 1 pointwise.\n",
    "\n",
    "Whenever you see a CNN diagram with a skinny ‚Üí fat ‚Üí skinny channel pattern,\n",
    "a 1 √ó 1 convolution is doing the slimming or bulking.\n",
    "\n",
    "Add it to your toolbox when you need to:\n",
    "\n",
    "* Cut GPU memory without shrinking spatial size.  \n",
    "* Blend information across channels cheaply.  \n",
    "* Add extra non-linearity between big convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608fa72",
   "metadata": {},
   "source": [
    "## Practice Section <a id=\"sec-exercises\"></a>\n",
    "\n",
    "Now it's your turn! The following exercises will guide you through building a CNN from scratch and then fine-tuning a pre-trained model. This will solidify your understanding of the concepts covered in the theory section.\n",
    "\n",
    "### Part 1: Build a CNN from Scratch for MNIST\n",
    "\n",
    "In this part, you will define your own simple CNN, write a training loop, and train it to classify handwritten digits from the famous MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1: Load and Visualize MNIST Data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "# The values (0.1307,) and (0.3081,) are the global mean and standard deviation of the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Visualize a batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(4, 5, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(str(labels[idx].item()))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2acc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 ‚Äì Define the CNN Architecture  (all ops inside nn.Sequential)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ üîß YOUR CODE HERE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # Build ONE nn.Sequential called `self.features`.\n",
    "        #   Suggested order:  Conv ‚Üí ReLU ‚Üí MaxPool ‚Üí Conv ‚Üí ReLU ‚Üí MaxPool\n",
    "        self.features = nn.Sequential(\n",
    "            # nn.Conv2d(1, 10, kernel_size=3, padding=1),\n",
    "            # ...\n",
    "            \n",
    "        )\n",
    "        # For classificarion you need to flatten the output\n",
    "        # Then use a linear layer or two with ReLU and finally a linear layer for the output with 10 classes\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # ...\n",
    "            # \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Simply run the two sequences defined above\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model_scratch = SimpleCNN()\n",
    "print(model_scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3: Write the Training and Validation Loop\n",
    "\n",
    "def train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # YOUR CODE HERE: Implement the training step for one batch.\n",
    "            # 1. Zero the gradients\n",
    "            # 2. Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            # 3. Calculate the loss\n",
    "            # 4. Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            # 5. Perform a single optimization step (parameter update)\n",
    "            \n",
    "            \n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "              f\"Training Loss: {running_loss/len(train_loader):.3f}.. \"\n",
    "              f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "    \n",
    "    print(\"Finished Training\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4: Train the Model\n",
    "\n",
    "# Instantiate the model\n",
    "model_scratch = SimpleCNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_scratch.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model (this will take a few minutes)\n",
    "trained_model_scratch = train_and_validate(model_scratch, train_loader, test_loader, criterion, optimizer, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5: Save, Load, and Visualize Model Predictions\n",
    "\n",
    "# Save the trained model checkpoint\n",
    "torch.save(trained_model_scratch.state_dict(), \"mnist_cnn.pth\")\n",
    "print(\"Model checkpoint saved as mnist_cnn.pth\")\n",
    "\n",
    "# Load the model checkpoint into a new instance \n",
    "loaded_model = SimpleCNN()\n",
    "loaded_model.load_state_dict(torch.load(\"mnist_cnn.pth\"))\n",
    "loaded_model.eval()\n",
    "\n",
    "# Get a batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(images)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "# Plot the first 10 test images, their predicted labels, and true labels\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "plt.subplots_adjust(hspace=0.6)  # Increase vertical space between rows\n",
    "for idx in np.arange(10):\n",
    "    ax = fig.add_subplot(2, 5, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(f\"Pred: {preds[idx].item()}\\nTrue: {labels[idx].item()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8fa7c",
   "metadata": {},
   "source": [
    "### Part 2: Use pretrained models from timm library\n",
    "\n",
    "In this part, you will use a powerful repository called timm (PyTorch Image Models). It provides access to hundreds of pretrained models.\n",
    "\n",
    "Note: If you don't have a GPU, the code below may take a considerable amount of time to run. To monitor progress, you can print the loss values after each batch (by adjusting your *train_and_validate* function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Load a pre-trained backbone and adapt it to 10 CIFAR classes\n",
    "model_timm = timm.create_model(\"resnet18\", pretrained=True, num_classes=10)\n",
    "model_timm.to(device)\n",
    "\n",
    "# (Optional) freeze everything except the classifier head\n",
    "for name, p in model_timm.named_parameters():\n",
    "    if not name.startswith(\"fc\"):   # ResNet's final layer = \"fc\"\n",
    "        p.requires_grad = False\n",
    "\n",
    "# 2. Data ‚Äî CIFAR-10 images resized to 224√ó224 expected by ImageNet models\n",
    "transform_ft = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_set_ft = datasets.CIFAR10(\"./data\", train=True,  download=False, transform=transform_ft)\n",
    "test_set_ft  = datasets.CIFAR10(\"./data\", train=False, download=False, transform=transform_ft)\n",
    "\n",
    "train_loader_ft = DataLoader(train_set_ft, batch_size=64, shuffle=True)\n",
    "test_loader_ft  = DataLoader(test_set_ft,  batch_size=256, shuffle=False)\n",
    "\n",
    "# 3. Loss & Optimizer (only train unfrozen params)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_timm.parameters()), lr=1e-3)\n",
    "\n",
    "# 4. Quick sanity check\n",
    "imgs, _ = next(iter(train_loader_ft))\n",
    "print(\"batch:\", imgs.shape, \"‚Üí logits:\", model_timm(imgs.to(device)).shape)\n",
    "\n",
    "# YOUR CODE HERE ------------------------------------------------------------------\n",
    "# Re-use your `train_and_validate` function:\n",
    "#\n",
    "trained_timm = train_and_validate(model_timm,\n",
    "                                  train_loader_ft,\n",
    "                                  test_loader_ft,\n",
    "                                  criterion,\n",
    "                                  optimizer,\n",
    "                                  epochs=3)\n",
    "\n",
    "\n",
    "# Try:\n",
    "#   ‚Ä¢ Un-freezing more layers and observe accuracy changes.\n",
    "#   ‚Ä¢ Different timm models: \"tf_efficientnet_b0\", \"mobilenetv3_small_100\", \"vit_base_patch16_224\", ‚Ä¶\n",
    "#   ‚Ä¢ Tweaking learning-rate / batch-size / augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66c9afd",
   "metadata": {},
   "source": [
    "üéâ **Congratulations!**  \n",
    "You now have 2 working classifiers.\n",
    "\n",
    "Try experimenting with different architectures, data‚Äëaugmentation strategies, optimizers, or learning‚Äërate schedules to push the accuracy even higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
