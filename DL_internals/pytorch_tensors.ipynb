{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tensors \n",
    "Welcome! In this notebook you will learn what a **tensor** is, how to create and inspect tensors, and finally how to build a very small neural network (**MLP**) using torch. Each topic has a short text you can read first, then a code cell you can run.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hyperskill-content/hyperskill-ml-notebooks/blob/main/pytorch_tensors.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Prerequisites\n",
    "\n",
    "Make sure you’re comfortable with the topics below before starting this notebook:\n",
    "\n",
    "| # | Topic (clickable links)|\n",
    "|---|-------|\n",
    "| 1 | **[Introduction to PyTorch](https://hyperskill.org/learn/step/34436)**  |\n",
    "| 2 | **[Basics of neural network architecture](https://hyperskill.org/learn/step/26997)** |\n",
    "| 3 | **[Activation functions](https://hyperskill.org/learn/step/35741)** | \n",
    "| 4 | **[Multilayer perceptron](https://hyperskill.org/learn/step/47283)** | \n",
    "\n",
    "*If you’re new to any item above, review it quickly, then dive back in here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📑 Table of Contents\n",
    "\n",
    "1. [What Is a Tensor?](#sec-tensor)  \n",
    "2. [Inspecting a Tensor](#sec-inspect)  \n",
    "3. [Creating Tensors](#sec-create)  \n",
    "4. [Indexing & Slicing](#sec-index)  \n",
    "5. [Reshaping & Transposing](#sec-reshape)  \n",
    "6. [In-Place vs Out-of-Place Ops](#sec-inplace)  \n",
    "7. [Broadcasting](#sec-broadcast)  \n",
    "8. [Mini Exercise — RGB to Grayscale](#sec-grayscale)  \n",
    "9. [Tiny MLP](#sec-mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What Is a Tensor?\n",
    "<a id=\"sec-tensor\"></a>\n",
    "A **tensor** is simply a multidimensional array of numbers.  \n",
    "* A single number is a **0-D tensor** (scalar).  \n",
    "* A list like `[1, 2, 3]` is a **1-D tensor** (vector).  \n",
    "* A table of numbers is a **2-D tensor** (matrix).  \n",
    "PyTorch stores tensors efficiently and lets us move them to GPU for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar: tensor(7)\n",
      "vector: tensor([1, 2, 3])\n",
      "matrix:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(7)                # 0-D\n",
    "vector = torch.tensor([1, 2, 3])        # 1-D\n",
    "matrix = torch.tensor([[1, 2], [3, 4]]) # 2-D\n",
    "print(\"scalar:\", scalar)\n",
    "print(\"vector:\", vector)\n",
    "print(\"matrix:\\n\", matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspecting a Tensor — First Things to Check\n",
    "<a id=\"sec-inspect\"></a>\n",
    "Before any operation, ask three quick questions:\n",
    "\n",
    "| Property | Why you need it | How to get it |\n",
    "|----------|-----------------|---------------|\n",
    "| **shape** | does it fit the layer? | `x.shape` |\n",
    "| **dtype** | float, int, bool? | `x.dtype` |\n",
    "| **device** | CPU or GPU? | `x.device` |\n",
    "\n",
    "Bonus helpers:  \n",
    "* `x.ndim` → number of dimensions  \n",
    "* `x.numel()` → total elements  \n",
    "* `x.stride()` → memory step size per dim (helps with speed issues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape : torch.Size([2, 3, 4])\n",
      "dtype : torch.float32\n",
      "device: cpu\n",
      "ndim  : 3\n",
      "numel : 24\n",
      "stride: (12, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4)\n",
    "print(\"shape :\", x.shape)\n",
    "print(\"dtype :\", x.dtype)\n",
    "print(\"device:\", x.device)\n",
    "print(\"ndim  :\", x.ndim)\n",
    "print(\"numel :\", x.numel())\n",
    "print(\"stride:\", x.stride())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating Tensors — Your Toolkit\n",
    "<a id=\"sec-create\"></a>\n",
    "\n",
    "Use these factory functions every day:\n",
    "\n",
    "| Function | Typical use |\n",
    "|----------|-------------|\n",
    "| `torch.zeros` | initialise weights / masks |\n",
    "| `torch.ones`  | add bias of ones |\n",
    "| `torch.empty` | reserve memory fast (values uninitialised) |\n",
    "| `torch.full`  | constant tensors (e.g. padding value) |\n",
    "| `torch.arange`, `torch.linspace` | index vectors, positional encodings |\n",
    "\n",
    "*Real-world tip:* Random initialisation matters — try different seeds when models misbehave.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "range:\n",
      " tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "ones:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "linspace:\n",
      " tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
      "full:\n",
      " tensor([[42, 42, 42],\n",
      "        [42, 42, 42]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.zeros(2, 3)\n",
    "o = torch.ones(2, 3)\n",
    "r = torch.arange(0, 6).reshape(2, 3)\n",
    "l = torch.linspace(0, 1, steps=5)\n",
    "c = torch.full((2, 3), 42)\n",
    "print(\"zeros:\\n\", z)\n",
    "print(\"range:\\n\", r)\n",
    "print(\"ones:\\n\", o)\n",
    "print(\"linspace:\\n\", l)\n",
    "print(\"full:\\n\", c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Indexing & Slicing — Grabbing What You Need\n",
    "<a id=\"sec-index\"></a>\n",
    "\n",
    "Like NumPy, but with gradients.\n",
    "\n",
    "* Row / column selection  \n",
    "* Boolean masks for filtering  \n",
    "* Concatenation (`torch.cat`) and stacking (`torch.stack`) to merge data  \n",
    "  *Use case:* building mini-batches from separate tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "row 1 : tensor([4, 5, 6, 7])\n",
      "col 0 : tensor([0, 4, 8])\n",
      "evens : tensor([ 0,  2,  4,  6,  8, 10])\n",
      "cat shape: torch.Size([6, 4])\n",
      "stack shape: torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(12).reshape(3, 4)\n",
    "print(\"a:\\n\", a)\n",
    "print(\"row 1 :\", a[1])          # second row\n",
    "print(\"col 0 :\", a[:, 0])       # first column\n",
    "mask = a % 2 == 0\n",
    "print(\"evens :\", a[mask])       # boolean filter\n",
    "\n",
    "b = torch.ones_like(a)\n",
    "cat = torch.cat([a, b], dim=0)  # join rows\n",
    "stk = torch.stack([a, b], dim=1)# add new dim\n",
    "print(\"cat shape:\", cat.shape)\n",
    "print(\"stack shape:\", stk.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Reshaping & Transposing — Same Data, New View\n",
    "<a id=\"sec-reshape\"></a>\n",
    "\n",
    "* `view`, `reshape` → change shape without moving data (fast)  \n",
    "* `transpose`, `permute` → reorder dimensions  \n",
    "* `.contiguous()` → make memory continuous if needed\n",
    "\n",
    "**Use case:** Flatten images before a linear layer or swap channels for OpenCV (torch uses RGB and opencv BGR channel orders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix:\n",
      " tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "transposed:\n",
      " tensor([[0, 4],\n",
      "        [1, 5],\n",
      "        [2, 6],\n",
      "        [3, 7]])\n",
      "is contiguous: False\n",
      "flattened: tensor([0, 4, 1, 5, 2, 6, 3, 7])\n"
     ]
    }
   ],
   "source": [
    "v = torch.arange(8)\n",
    "m = v.view(2, 4)        # 2×4\n",
    "t = m.T                 # 4×2\n",
    "print(\"matrix:\\n\", m)\n",
    "print(\"transposed:\\n\", t)\n",
    "print(\"is contiguous:\", t.is_contiguous())\n",
    "t_c = t.contiguous().view(-1)\n",
    "print(\"flattened:\", t_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6  In-Place (`*_`) vs. Out-of-Place Operations\n",
    "<a id=\"sec-inplace\"></a>\n",
    "\n",
    "* **Out-of-place** – creates a **new tensor**, original stays the same.  \n",
    "* **In-place** – ends with an underscore `_` and **changes data inside the same tensor**.  \n",
    "\n",
    "In-place saves memory but can break Autograd if you try it on a **leaf tensor** (a tensor you created with `requires_grad=True`).  \n",
    "PyTorch will stop you with the error:\n",
    "```\n",
    "RuntimeError: a leaf Variable that requires grad is being used in an in-place operation\n",
    "```\n",
    "Below are quick demos:\n",
    "\n",
    "1. Safe out-of-place  \n",
    "2. In-place error on a leaf tensor  \n",
    "3. Two safe work-arounds  \n",
    "4. In-place on a non-leaf tensor (works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2., 3.], requires_grad=True)\n",
      "y: tensor([2., 3., 4.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣  Safe out-of-place\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = x + 1          # new tensor\n",
    "print(\"x:\", x)     # [1., 2., 3.]\n",
    "print(\"y:\", y)     # [2., 3., 4.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 2️⃣  In-place on a leaf tensor → error\u001b[39;00m\n\u001b[1;32m      2\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.\u001b[39m, \u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m3.\u001b[39m], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m z\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "# 2️⃣  In-place on a leaf tensor → error\n",
    "z = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "z.add_(1)          # raises RuntimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: tensor([2., 3., 4.], requires_grad=True)\n",
      "u_det: tensor([2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "# 3️⃣  Safe in-place methods\n",
    "\n",
    "# 3a. Use torch.no_grad()\n",
    "w = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    w.add_(1)\n",
    "print(\"w:\", w)     # [2., 3., 4.]\n",
    "\n",
    "# 3b. Detach first\n",
    "u = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "u_det = u.detach()\n",
    "u_det.add_(1)\n",
    "print(\"u_det:\", u_det)   # [2., 3., 4.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root.grad: tensor([2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "# 4️⃣  In-place on a non-leaf tensor (allowed)\n",
    "root  = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "child = root * 2            # not a leaf\n",
    "child.add_(1)               # in-place ok\n",
    "out = child.sum()\n",
    "out.backward()\n",
    "print(\"root.grad:\", root.grad)   # tensor([2., 2., 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key points**\n",
    "\n",
    "* You can always use out-of-place ops—they’re simple and safe, unless you have a memory limitations.\n",
    "* If you really need in-place, wrap it in `torch.no_grad()` or use `.detach()`.  \n",
    "* PyTorch optimisers already handle in-place parameter updates for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Broadcasting — Math with Different Shapes\n",
    "<a id=\"sec-broadcast\"></a>\n",
    "Broadcasting lets PyTorch stretch a smaller tensor to match a larger one.\n",
    "\n",
    "**Use case:** Add channel-wise mean / std when normalising images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9916, 0.7508, 0.4694],\n",
      "        [1.3362, 1.4189, 0.0522]])\n"
     ]
    }
   ],
   "source": [
    "mat = torch.rand(2, 3)\n",
    "vec = torch.tensor([1.0, 0.5, 0.0])   # shape (3,)\n",
    "res = mat + vec                       # vec → (2,3)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Mini Exercise — From RGB to Grayscale\n",
    "<a id=\"sec-grayscale\"></a>\n",
    "\n",
    "Goal:  \n",
    "*Take a batch of RGB images, convert each one to a single-channel grayscale image, then check the new shape.*\n",
    "\n",
    "Why it matters:  \n",
    "Many computer-vision tasks (e.g., edge detection, classic ML models) need **1-channel** input instead of color.  \n",
    "Knowing how to reduce channels is a handy tensor skill.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape: torch.Size([16, 3, 128, 128])\n",
      "grayscale shape: torch.Size([16, 1, 128, 128])\n",
      "min/max: 0.010739028453826904 0.9929993152618408\n"
     ]
    }
   ],
   "source": [
    "# Batch of 16 RGB images with pixel values in [0, 1]\n",
    "imgs = torch.rand(16, 3, 128, 128)        # shape (B, C, H, W)\n",
    "\n",
    "# Average the 3 colour channels → grayscale\n",
    "gray = imgs.mean(dim=1, keepdim=True)     # shape becomes (B, 1, H, W)\n",
    "\n",
    "print(\"original shape:\", imgs.shape)\n",
    "print(\"grayscale shape:\", gray.shape)\n",
    "\n",
    "# Optional: verify range still [0, 1]\n",
    "print(\"min/max:\", gray.min().item(), gray.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ Tiny MLP — How It Works\n",
    " \n",
    "<a id=\"sec-mlp\"></a>\n",
    "\n",
    "A **Multilayer Perceptron (MLP)** is the hello-world of neural networks.  \n",
    "Ours has three main parts:\n",
    "\n",
    "| Part | What it does | Why it matters |\n",
    "|------|--------------|----------------|\n",
    "| **Flatten layer** | Turns a 3-D image `(C × H × W)` into a 1-D vector | Linear layers need 1-D input |\n",
    "| **Linear + ReLU (hidden layer)** | `x → Wx + b`, then keep only positive values | Learns mixed features (edges, colours, …) |\n",
    "| **Linear (output layer)** | New weights turn hidden features into **logits** (raw scores) | One logit per class (10 here) |\n",
    "| **Softmax (optional)** | Converts logits to probabilities that sum to 1 | Easier to read & plot |\n",
    "\n",
    "> **Why just two layers?**  \n",
    "> Small models train fast and make debugging easy. Once the flow is clear, you can add more layers or switch to ConvNets.\n",
    "\n",
    "In the demo below you will:\n",
    "\n",
    "1. Watch shapes change through the network  \n",
    "2. Turn logits into probabilities  \n",
    "3. Visualise the predicted class distribution for one sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input   : torch.Size([4, 3, 224, 224])\n",
      "flatten : torch.Size([4, 150528])\n",
      "hidden  : torch.Size([4, 128])\n",
      "logits  : torch.Size([4, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----- Tiny MLP definition -----\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self, in_features=3*224*224, hidden=128, out=10):\n",
    "        super().__init__()\n",
    "        self.flat = nn.Flatten()              # 1️⃣ flatten\n",
    "        self.hidden = nn.Linear(in_features, hidden)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Linear(hidden, out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"input   :\", x.shape)            # (B, 3, 224, 224)\n",
    "        x = self.flat(x)\n",
    "        print(\"flatten :\", x.shape)            # (B, 150 528)\n",
    "        x = self.act(self.hidden(x))\n",
    "        print(\"hidden  :\", x.shape)            # (B, 128)\n",
    "        logits = self.out(x)\n",
    "        print(\"logits  :\", logits.shape)       # (B, 10)\n",
    "        return logits\n",
    "\n",
    "# ----- Fake batch -----\n",
    "batch = torch.rand(4, 3, 224, 224)            # 4 RGB images\n",
    "\n",
    "model = TinyMLP()\n",
    "logits = model(batch)                         # forward pass\n",
    "probs  = logits.softmax(dim=1)                # nice probabilities\n",
    "\n",
    "# ----- Visualise probs for the first sample -----\n",
    "classes = [f\"C{i}\" for i in range(10)]\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(classes, probs[0].detach().numpy())\n",
    "plt.title(\"Class probabilities (sample 0)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1  Autograd in Action — One Training Step\n",
    "\n",
    "`Autograd` is PyTorch’s automatic-differentiation engine.  \n",
    "When you call `.backward()`, PyTorch walks back through the graph and fills `.grad` for each parameter.\n",
    "\n",
    "Below we will:\n",
    "\n",
    "1. Create **fake labels**.  \n",
    "2. Compute **cross-entropy loss** on the logits from the Tiny MLP.  \n",
    "3. Call `loss.backward()` and inspect one gradient tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.3891186714172363\n",
      "hidden.weight.grad shape: torch.Size([128, 150528])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- fake labels (0-9) -----\n",
    "targets = torch.randint(0, 10, (logits.size(0),))       # shape (B,)\n",
    "\n",
    "# ----- loss & backward -----\n",
    "loss = F.cross_entropy(logits, targets)\n",
    "print(\"loss:\", loss.item())\n",
    "\n",
    "loss.backward()    # gradients now stored in model parameters\n",
    "\n",
    "# show gradient of first weight matrix\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(f\"{name}.grad shape:\", p.grad.shape)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2  Moving Data & Model to GPU\n",
    "\n",
    "If you have an NVIDIA GPU, you can run the same network **much faster**.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Pick a device: `\"cuda\"` if available, else `\"cpu\"`.  \n",
    "2. `.to(device)` on **both** data **and** model.  \n",
    "3. Run the forward pass.  (Optionally, time it.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "input   : torch.Size([8, 3, 224, 224])\n",
      "flatten : torch.Size([8, 150528])\n",
      "hidden  : torch.Size([8, 128])\n",
      "logits  : torch.Size([8, 10])\n",
      "output shape: torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# move model and a new dummy batch\n",
    "model_gpu = TinyMLP().to(device)\n",
    "batch_gpu  = torch.rand(8, 3, 224, 224, device=device)\n",
    "\n",
    "# quick timing\n",
    "with torch.no_grad():\n",
    "    logits_gpu = model_gpu(batch_gpu)        # forward on GPU / CPU\n",
    "print(\"output shape:\", logits_gpu.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3  Putting It All Together\n",
    "\n",
    "* You now know how to **create a model**, **move it to GPU**,  \n",
    "  **run a forward pass**, **compute loss**, and **back-propagate gradients**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 – Build a 2 × 3 Tensor of Zeros  \n",
    "\n",
    "Create a tensor named **`t`** with shape `(2, 3)`, all zeros, `dtype=torch.float32`, living on the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 🚧 Your code below\n",
    "# t = ...\n",
    "\n",
    "# 🔎 Tests\n",
    "assert isinstance(t, torch.Tensor), \"Not a tensor\"\n",
    "assert t.shape == (2, 3), \"Shape should be (2, 3)\"\n",
    "assert t.dtype == torch.float32, \"dtype must be float32\"\n",
    "assert t.device.type == \"cpu\", \"Keep it on CPU for this test\"\n",
    "assert torch.all(t == 0), \"Tensor must contain only zeros\"\n",
    "print(\"✅ Exercise 1 passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 – Reshape a Vector to 3 × 4  \n",
    "\n",
    "Given the 1-D tensor `v` below, create `m` with shape `(3, 4)` **using `view`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.arange(12)           # [0 … 11]\n",
    "# 🚧 Your code below\n",
    "# m = ...\n",
    "\n",
    "# 🔎 Tests\n",
    "assert m.shape == (3, 4), \"Shape should be (3, 4)\"\n",
    "assert m.is_contiguous(), \"Use view, not transpose\"\n",
    "assert torch.equal(m.flatten(), v), \"Values must match original order\"\n",
    "print(\"✅ Exercise 2 passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 – Stack Along a New Dimension  \n",
    "\n",
    "Stack `a` and `b` **along a new leading dimension** so the result has shape `(2, 3, 4)`.  \n",
    "(Do **not** use a Python list; use `torch.stack`.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3, 4)\n",
    "b = torch.zeros(3, 4)\n",
    "# 🚧 Your code below\n",
    "# out = ...\n",
    "\n",
    "# 🔎 Tests\n",
    "assert out.shape == (2, 3, 4), \"Result should be (2, 3, 4)\"\n",
    "assert (out[0] == 1).all() and (out[1] == 0).all(), \"Order should be [a, b]\"\n",
    "print(\"✅ Exercise 3 passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4 – Convert to Grayscale  \n",
    "\n",
    "Given a batch `imgs` of shape `(N, 3, H, W)` with values in `[0, 1]`,  \n",
    "create **`gray`** with shape `(N, 1, H, W)` by averaging the colour channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.rand(5, 3, 64, 64)   # fake batch\n",
    "# 🚧 Your code below\n",
    "# gray = ...\n",
    "\n",
    "# 🔎 Tests\n",
    "assert gray.shape == (5, 1, 64, 64), \"Wrong shape\"\n",
    "assert (gray >= 0).all() and (gray <= 1).all(), \"Keep values in [0, 1]\"\n",
    "# spot-check mean\n",
    "assert torch.allclose(gray[0,0,0,0], imgs[0,:,0,0].mean()), \"Averaging mismatch\"\n",
    "print(\"✅ Exercise 4 passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5 – Device Transfer  \n",
    "\n",
    "Move tensor `x` and the TinyMLP `model` to **`cuda`** *if* a GPU is available,  \n",
    "otherwise leave them on CPU.  Store the result in `x_dev` and `model_dev`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 224, 224)\n",
    "model = TinyMLP()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 🚧 Your code below\n",
    "# x_dev = ...\n",
    "# model_dev = ...\n",
    "\n",
    "# 🔎 Tests\n",
    "assert x_dev.device == device, \"x_dev on wrong device\"\n",
    "assert next(model_dev.parameters()).device == device, \"model_dev on wrong device\"\n",
    "print(\"✅ Exercise 5 passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "To work confidently with data in PyTorch, start by **creating tensors** with helper functions such as `torch.zeros`, `torch.ones`, or `torch.rand`, then **inspect** them with `.shape`, `.dtype`, and `.device` to catch shape or type errors early.  \n",
    "When you need a new view of the same data, use `view` / `reshape`; if you must reorder dimensions, rely on `transpose` or `permute`, and call `.contiguous()` before reshaping if the stride is disrupted.\n",
    "\n",
    "Slice and index tensors just as you would in NumPy, or merge them with `torch.cat` and `torch.stack` to build mini-batches.  \n",
    "Remember that **out-of-place operations** create new tensors and are always safe, while **in-place operations** (ending in `_`) save memory but cannot be applied to leaf tensors that require gradients without wrapping them in `torch.no_grad()` or detaching first.\n",
    "\n",
    "Use **broadcasting** to let PyTorch align smaller tensors automatically—ideal for channel-wise operations like converting an RGB batch to a single-channel grayscale batch by averaging along `dim=1`.\n",
    "\n",
    "After preprocessing, **flatten images** before a `Linear` layer, pass them through a hidden layer with `ReLU`, and produce logits with an output layer to form a **tiny MLP**.  \n",
    "Compute loss with `torch.nn.functional.cross_entropy`, call `loss.backward()` to fill parameter gradients, and move both data and model to `\"cuda\"` when a GPU is available to accelerate training.\n",
    "\n",
    "These steps—tensor creation, inspection, reshaping, safe memory operations, broadcasting, simple preprocessing, Autograd, and GPU transfer—form the everyday toolkit for building and debugging neural-network pipelines in PyTorch.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
